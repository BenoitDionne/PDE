\chapter{Review of some Special Functions}\label{ChapSpecFunct}

In this chapter, we very briefly present some special functions which
are used to express the general solutions of linear second order
partial differential equations like the heat equation, the wave
equation and the potential equation.  Good references for the
material of this chapter are \cite{Simm,RenRog}.

The ``Legendre polynomials'' that we will introduce later are defined
as the solutions of a specific differential equation found using the
``Frobenius method'' near a ``regular point.''\ Likewise,
the Bessel functions that we will also introduce later are defined
as the solutions of a specific differential equation found using the
``Frobenius method'' near a ``regular singular point.''\
We begin by presenting the Frobenius method.

\section{Frobenius Method}

Under what conditions is the solution $y$ of an ordinary differential
equation analytic about a point $a$?  Namely, under what conditions do
we have $\displaystyle y = \sum_{n=0}^\infty  a_n (x-a)^n$, a series
with a positive radius of convergence, as a solution of an ordinary
differential equation?  The following example shows that we cannot
always assume that this is possible.

\begin{egg}
Consider the ordinary differential equation
\begin{equation}\label{series_fails}
x^2 y' + (x-1)y + 1 = 0 \ .
\end{equation}
Suppose that we have a solution of the form
$\displaystyle y = \sum_{n=0}^\infty a_n x^n$ with a positive radius
of convergence.  Then
$\displaystyle y' = \sum_{n=1}^\infty n a_n x^{n-1}$ on the interval
of convergence.  If we substitute these series for $y$ and $y'$ in
(\ref{series_fails}), we get
\begin{align*}
0 &= \sum_{n=1}^\infty n a_n x^{n+1} +(x-1) \sum_{n=0}^\infty a_n x^n + 1
= \sum_{n=1}^\infty n a_n x^{n+1} + \sum_{n=0}^\infty a_n x^{n+1} -
\sum_{n=0}^\infty a_n x^n + 1 \\
&= \sum_{n=2}^\infty (n-1) a_{n-1} x^n + \sum_{n=1}^\infty a_{n-1} x^n -
\sum_{n=0}^\infty a_n x^n + 1 \\
&= \sum_{n=2}^\infty \left( n a_{n-1} - a_n\right) x^n
+ (a_0 - a_1)x - a_0 + 1 \ .
\end{align*}
Hence, we get $a_0 = 1$ from $1-a_0 = 0$,
$a_1 = a_0 = 1$ from $a_0 - a_1 = 0$,
$a_2 = 2a_1 = 2$ from $2 a_1 - a_2 = 0$,
$a_3 = 3a_2 = 3\cdot 2 = 3!$ from $3 a_2 - a_3 = 0$, and so on.
If we assume that $a_{n-1} = (n-1)!$, then we get $a_n = n!$ from
$n a_{n-1} - a_n = 0$.  By induction, we have that $a_n = n!$ for all
non-negative integers.  We find that
$\displaystyle y = \sum_{n=0}^\infty n! x^n$.  The ratio test
shows that this series diverges for $x\neq 0$.  Therefore, we cannot
assume that there is an analytic solution of (\ref{series_fails}) in
the neighbourhood of the origin.
\end{egg}

Consider ordinary differential equations of the form
\begin{equation} \label{spv_frobenius}
y'' + P(x) y' + Q(x) y = 0 \ .
\end{equation}
If $P(x)$ and $Q(x)$ are analytic functions at $x=0$, we say that
$0$ is a {\bfseries regular point}\index{Regular Point}.
If $x P(x)$ and $\displaystyle x^2 Q(x)$ are analytic functions at
$x=0$, we say that $0$ is a
{\bfseries regular singular point}\index{Regular Singular Point}.

Suppose that $0$ is a regular singular point; say
$\displaystyle x P(x) = \sum_{j=0}^\infty \alpha_j x^j$ and
$\displaystyle x^2 Q(x) = \sum_{j=0}^\infty \beta_j x^j$ for $|x| < R$.
Moreover, suppose $\lambda_1$ and $\lambda_2$ are the roots of the
{\bfseries indicial equation}\index{Indicial Equation}
$F(\lambda) = \lambda^2 + (\alpha_0 - 1) \lambda + \beta_0 = 0$
with $\lambda_1 \geq \lambda_2$.

One solution of the ordinary differential equation is given by
\[
y_1(x) = x^{\lambda_1}(a_0 + a_1 x + a_2 x^2 + \ldots) \ ,
\]
where the series converges absolutely for $|x|<R$.  A second linearly
independent solution of the ordinary differential equation is given by
\begin{equation} \label{spv_FMchoices}
y_2(x) =
\begin{cases}
x^{\lambda_2} ( b_0 + b_1 x + b_2 x^2 + \ldots ) & \text{ if } 
\lambda_1 - \lambda_2 \not\in \NN \\
y_1(x) \ln(x) + x^{\lambda_2} ( b_1 x + b_2 x^2 + \ldots ) &
\text{ if } \lambda_1 = \lambda_2 \\
k y_1(x) \ln(x) + x^{\lambda_2} ( b_0 + b_1 x + b_2 x^2 + \ldots ) &
\text{ if } 0<\lambda_1 - \lambda_2 \in \NN
\end{cases}
\end{equation}
where again the series converges absolutely for $|x|<R$ and $k$ is a constant.

There are some situations where we may avoid manipulating the
formula for $y_2$ which has a term in $\ln(x)$.  If $y_1$ is a known
function, we can use the method of reduction of order to find a
second linearly independent solution; it suffices to replace $y$ by
$y_2(x) = y_1(x) u(x)$ in the ordinary differential equation and solve for $u$.

If $0<s = \lambda_1-\lambda_2 \in \NN$, the solution $y_2$ with $k=0$
gives a second linearly independent solution when $b_0$ and $b_s$ are
free.

There is also a method to find $y_2$ when $\lambda_1=\lambda_2$ which
does not require the expression with the $\ln(x)$.  To find the
first series solution $y_1$, we substitute the series
\begin{equation} \label{spv_frobenius2}
y(t,\lambda) = x^\lambda \sum_{n=0}^\infty c_n x^n 
\end{equation}
in (\ref{spv_frobenius}) to get an expression of the form
\begin{equation} \label{spv_frobenius3}
c_0 F(\lambda) x^\lambda + x^\lambda \sum_{n=1}^\infty
\left(c_n F(n+\lambda) +
G(\lambda,c_{n-p},c_{n-p+1},\ldots,c_{n-1})\right) x^n = 0 \  ,
\end{equation}
where $G$ is some function, $p>0$ is a fixed integer and $c_j = 0$ for
$j<0$.  If $\lambda=\lambda_1$, then $c_0$ is free and the recurrence
relation
\[
c_n = \frac{G(\lambda_1,c_{n-p},c_{n-p+1},\ldots,c_{n-1})}
{F(n+\lambda_1)}
\]
for $n\geq 1$ gives all the other coefficients of the solution.  When
$\lambda_1=\lambda_2$, the indicial equation is
$\displaystyle F(\lambda) = (\lambda-\lambda_1)^2$.  If we substitute
(\ref{spv_frobenius2}) with
\[
c_n = \frac{G(\lambda,c_{n-p},c_{n-p+1},\ldots,c_{n-1})}{F(n+\lambda)}
\]
for $n \geq 1$ in (\ref{spv_frobenius}), we get from
(\ref{spv_frobenius3}) that
\[
\dydxn{y}{x}{2}(x,\lambda) + P(x) \dydx{y}{x}(x,\lambda)
+ Q(x) y(x,\lambda) = c_0(\lambda-\lambda_1)^2x^{\lambda} \ .
\]
If we derive this relation with respect to $\lambda$, we get
\begin{align*}
& \dfdxn{\left( \pdydx{y}{\lambda}(x,\lambda) \right)}{x}{2}
+ P(x) \dfdx{\left( \pdydx{y}{\lambda}(x,\lambda) \right)}{x}
+ Q(x) \left( \pdydx{y}{\lambda}(x,\lambda) \right) \\
&\qquad = c_0\left(2(\lambda-\lambda_1) + (\lambda-\lambda_1)^2\ln(x)\right)
x^{\lambda} \ .
\end{align*}
Hence $\displaystyle \pdydx{y}{\lambda}(x,\lambda)\big|_{\lambda=\lambda_1}$
is another solution of (\ref{spv_frobenius}) which is linearly
independent of $y_1$.

\begin{egg}
Solve the ordinary differential equation
\[
4xy''+ 2 y' + y = 0 \ .
\]

We have an equation of the form
$\displaystyle y'' + P(x)\, y' + Q(x) y = 0$, where
$\displaystyle P(x) = \frac{1}{2x}$ and
$\displaystyle Q(x) = \frac{1}{4x}$.
Thus $\displaystyle x P(x) = \frac{1}{2}$ and
$\displaystyle x^2 Q(x) = \frac{x}{4}$.  We find
$\alpha_0 = 1/2$ and $\beta_0= 0$.  The indicial equation is
$\displaystyle \lambda^2 + (\alpha_0 - 1)\lambda + \beta_0 = \lambda^2
-\frac{1}{2}\,\lambda  = \lambda \left(\lambda -\frac{1}{2}\right) = 0$.
Its roots are $\displaystyle \lambda_1 = \frac{1}{2}$ and $\lambda_2=0$.
Since $\lambda_1-\lambda_2$ is not an integer, we get two linearly
independent solutions
$\displaystyle y_1 = x^{\lambda_1} \sum_{n=0}^\infty a_n x^n$
and $\displaystyle y_2 = x^{\lambda_2} \sum_{n=0}^\infty b_n x^n$.

If we substitute $\displaystyle y = x^\lambda \sum_{n=0}^\infty c_n x^n$ into
$\displaystyle 4xy''+ 2 y' + y = 0$, we get
\begin{align*}
0 & = 4xy''+ 2 y' + y
= 4 \sum_{n=0}^\infty (\lambda+n)(\lambda+n-1) c_n
x^{\lambda+n-1} + 2 \sum_{n=0}^\infty (\lambda + n )c_n x^{\lambda+n-1}
+ \sum_{n=0}^\infty c_n x^{\lambda+n} \\
&= 4 \sum_{n=-1}^\infty (\lambda+n+1)(\lambda+n) c_{n+1}
x^{\lambda+n} + 2 \sum_{n=-1}^\infty (\lambda + n+1 )c_{n+1} x^{\lambda+n}
+ \sum_{n=0}^\infty c_n x^{\lambda+n} \\
&=  2\lambda(2\lambda-1) c_0 x^{\lambda-1} + \sum_{n=0}^\infty \big(
2(\lambda+n+1)(2\lambda+2n +1)c_{n+1} +c_n\big) x^{\lambda+n} \ .
\end{align*}
We get $2\lambda( 2\lambda -1)c_0 = 0$ and the recursion relation
$\displaystyle c_{n+1} = \frac{-1}{2(\lambda+n+1)(2\lambda+2n+1)} c_n$
for $n\geq 0$.

For $\lambda = 1/2$, we have that
$\displaystyle 2\lambda( 2\lambda -1)c_0 = 0 c_0 = 0$ whatever the
value of $c_0$.  Thus, $c_0$ is free.  The recursion relation becomes
$\displaystyle c_{n+1} = \frac{-1}{(2n+3)(2n+2)} c_n$
for $n \geq 0$.  We find
$\displaystyle c_1 = -\frac{c_0}{3\times 2} = - \frac{c_0}{3!}$,
$\displaystyle c_2 = - \frac{c_1}{5\times 4}
= \frac{c_0}{5\times 4 \times 3 \times 2} = \frac{c_0}{5!}$,
$\displaystyle c_3 = - \frac{c_2}{7\times 6}
= -\frac{c_0}{7\times 6\times 5\times 4 \times 3 \times 2} = - \frac{c_0}{7!}$,
and so on.  By induction, we find that
$\displaystyle c_n = (-1)^n \frac{c_0}{(2n+1)!}$ for $n \geq 0$.
The first solution is
\[
y_1 = c_0 \, x^{1/2} \sum_{n=0}^\infty (-1)^n \frac{x^n}{(2n+1)!}
= c_0 \, \sum_{n=0}^\infty (-1)^n \frac{(\sqrt{x})^{2n+1}}{(2n+1)!}
= c_0 \sin(\sqrt{x}) \ .
\]

For $\lambda = 0$, we have that
$\displaystyle 2\lambda( 2\lambda -1)c_0 = 0 c_0 = 0$ whatever the
value of $c_0$.  Thus, $c_0$ is also free.  The recursion relation becomes
$\displaystyle c_{n+1} = \frac{-1}{(2n+2)(2n+1)} c_n$
for $n \geq 0$.  We find
$\displaystyle c_1 = - \frac{c_0}{2\times 1} = - \frac{c_0}{2!}$,
$\displaystyle c_2 = - \frac{c_1}{4\times 3}
= \frac{c_0}{4 \times 3 \times 2} = \frac{c_0}{4!}$,
$\displaystyle c_3 = - \frac{c_2}{6\times 5}
= -\frac{c_0}{6\times 5\times 4 \times 3 \times 2} = - \frac{c_0}{6!}$,
and so on.  By induction, we find that
$\displaystyle c_n = (-1)^n \frac{c_0}{(2n)!}$ for $n\geq 0$.
A second solution linearly independent from the first one is
\[
y_2 = c_0 \, \sum_{n=0}^\infty (-1)^n \frac{x^n}{(2n)!}
= c_0 \, \sum_{n=0}^\infty (-1)^n \frac{(\sqrt{x})^{2n}}{(2n)!}
= c_0 \cos(\sqrt{x}) \ .
\]

The general solution is
$\displaystyle y = C\, \sin(\sqrt{x}) + D\, \cos(\sqrt{x})$ for
arbitrary constants $C$ and $D$.
\end{egg}

\begin{egg}
Solve the ordinary differential equation
\[
2xy''+  y' + x y = 0 \ .
\]

We have an equation of the form
$\displaystyle y'' + P(x)\, y' + Q(x) y = 0$, where
$\displaystyle P(x) = \frac{1}{2x}$ and
$\displaystyle Q(x) = 2$.
Thus $\displaystyle x P(x) = \frac{1}{2}$ and
$\displaystyle x^2 Q(x) = 2x^2$.  We find
$\alpha_0 = 1/2$ and $\beta_0= 0$.  The indicial equation is
$\displaystyle \lambda^2 + (\alpha_0 - 1)\lambda + \beta_0 = \lambda^2
-\frac{1}{2}\,\lambda  = \lambda \left(\lambda -\frac{1}{2}\right) = 0$.
Its roots are $\displaystyle \lambda_1 = 1/2$ and $\lambda_2=0$.
Since $\lambda_1-\lambda_2$ is not an integer, we have two linearly
independent solutions
$\displaystyle y_1 = x^{\lambda_1} \sum_{n=0}^\infty a_n x^n$
and $\displaystyle y_2 = x^{\lambda_2} \sum_{n=0}^\infty b_n x^n$.

If we substitute $\displaystyle y = x^\lambda \sum_{n=0}^\infty c_n x^n$ into
$\displaystyle 2xy''+  y' + xy = 0$, we get
\begin{align*}
0 & = 2xy''+ y' + xy
= 2 \sum_{n=0}^\infty (\lambda+n)(\lambda+n-1) c_n x^{\lambda+n-1}
+ \sum_{n=0}^\infty (\lambda + n )c_n x^{\lambda+n-1}
+ \sum_{n=0}^\infty c_n x^{\lambda+n+1} \\
&= 2 \sum_{n=-2}^\infty (\lambda+n+2)(\lambda+n+1) c_{n+2} x^{\lambda+n+1}
+ \sum_{n=-2}^\infty (\lambda + n+2 )c_{n+2} x^{\lambda+n+1}
+ \sum_{n=0}^\infty c_n x^{\lambda+n+1} \\
&= \lambda(2\lambda-1)c_0 x^{\lambda-1}
+ (2\lambda+1)(\lambda+1) c_1 x^{\lambda} \\
&\qquad + \sum_{n=0}^\infty \big( (\lambda+n+2)(2\lambda+2n+3) c_{n+2}
+c_n\big) x^{\lambda+n+1} \ .
\end{align*}
We get $\lambda(2\lambda -1)c_0 = 0$, $(2\lambda+1)(\lambda+1) c_1 = 0$
and the recursion relation
\[
  c_{n+2} = -\frac{1}{(\lambda+n+2)(2\lambda+2n+3)} c_n
\]
for $n \geq 0$.

For $\lambda = 1/2$, we have that
$\lambda( 2\lambda -1)c_0 = 0 c_0 = 0$ and
$(2\lambda+1)(\lambda+1) c_1 = 3 c_1 = 0$.
Thus, $c_0$ is free and $c_1=0$. The recursion relation becomes
$\displaystyle c_{n+2} = -\frac{1}{(n+2)(2n+5)} c_n$
for $n \geq 0$.  We find that
$\displaystyle c_3 = \frac{-1}{3\times 7} c_1 = 0$,
$\displaystyle c_5 = \frac{-1}{5\times 11} c_3 = 0$, and so on.
Thus, we get $c_{2n-1} = 0$ for $n>0$ by induction.
We also find that 
$\displaystyle c_2 = -\frac{1}{2\times 5}c_0$,
$\displaystyle c_4 = - \frac{1}{4\times 9}c_2
= \frac{c_0}{2\times 4 \times 5 \times 9}$,
$\displaystyle c_6 = - \frac{1}{6\times 13}c_4
= -\frac{c_0}{2\times 4\times 6\times 5 \times 9 \times 13}$, and so
on.  By induction, we get
\[
c_{2n} = (-1)^n \frac{c_0}{2^n n! \times 5 \times 9 \times 13 \times \ldots
\times (4n+1)}
\]
for $n >0$.  With $c_0=1$, we get a first solution
\[
y_1 = x^{1/2} 
\sum_{n=0}^\infty 
(-1)^n \frac{1}{2^n n! \times 5 \times 9 \times 13 \times \ldots
  \times (4n+1)}x^{2n} \ .
\]

For $\lambda = 0$, we have that
$\lambda( 2\lambda -1)c_0 = 0 c_0 = 0$ and
$(2\lambda+1)(\lambda+1) c_1 = c_1 = 0$.
Thus, $c_0$ is free and $c_1=0$. The recursion relation becomes
$\displaystyle c_{n+2} = -\frac{1}{(n+2)(2n+3)} c_n$
for $n \geq 0$.  We find that
$\displaystyle c_3 = \frac{-1}{3\times 5} c_1 = 0$,
$\displaystyle c_5 = \frac{-1}{5\times 9} c_3 = 0$, and so on.
Thus, we get $c_{2n-1} = 0$ for $n>0$ by induction.  We also find that
$\displaystyle c_2 = - \frac{c_0}{2\times 3}$,
$\displaystyle c_4 = - \frac{1}{4\times 7}c_2
= \frac{c_0}{2\times 4 \times 3 \times 7}$,
$\displaystyle c_6 = - \frac{1}{6\times 11}c_4
= -\frac{c_0}{2\times 4\times 6\times 3 \times 7 \times 11}$, and so on.
By induction, we get
\[
c_{2n} = (-1)^n \frac{c_0}{2^n n! \times 3 \times 7 \times 11 \times \ldots
\times (4n-1)}
\]
for $n > 0$.  With $c_0=1$, we get a second solution
linearly independent of the first one
\[
y_1 = 1+ \sum_{n=1}^\infty 
(-1)^n \frac{1}{2^n n! \times 3 \times 7 \times 11 \times \ldots
  \times (4n-1)}x^{2n} \ .
\]

The general solution is
$\displaystyle y = C\, y_1(x) + D\, y_2(x)$ for arbitrary constants
$C$ and $D$.
\end{egg}

\begin{egg}
Solve the ordinary differential equation $\displaystyle y''- y = 0$.

We have an equation of the form
$\displaystyle y'' + P(x)\, y' + Q(x) y = 0$, where
$\displaystyle P(x) = 0$ and $\displaystyle Q(x) = -1$.
Thus $\displaystyle x P(x) = 0$ and
$\displaystyle x^2 Q(x) = -x^2$.  We find $\alpha_0 = \beta_0= 0$.
The indicial equation is
$\displaystyle \lambda^2 + (\alpha_0 - 1) \lambda + \beta_0 =
\lambda^2 - \lambda = \lambda(\lambda -1) = 0$.
Its roots are $\displaystyle \lambda_1 = 1$ and $\lambda_2=0$.

Since $\lambda_1-\lambda_2$ is an integer, one solution is of the form
$\displaystyle y_1 = x \sum_{n=0}^\infty a_n x^n = \sum_{n=0}^\infty a_n x^{n+1}$
and another solution linearly independent of $y_1$ is of the form
$\displaystyle y_2 = k y_1 \ln(x) + \sum_{n=0}^\infty b_n x^n$.
However, as we will see, we are in the situation where we may take
$k=0$ in $y_2$ because $b_0$ and $b_1$ are free.  In this case, we
seek solutions of the form
$\displaystyle y = \sum_{n=0}^\infty c_n x^n$.
This will give us two linearly independent solutions.
If we substitute this expression of $y$ into $\displaystyle y''- y = 0$,
we get 
\begin{align*}
y''- y & = \sum_{n=2}^\infty n(n-1) c_n x^{n-2} - \sum_{n=0}^\infty c_n x^n
= \sum_{n=0}^\infty (n+1)(n+2) c_{n+2} x^n - \sum_{n=0}^\infty c_n x^n \\
&= \sum_{n=0}^\infty \left( (n+1)(n+2)c_{n+2} + c_n\right) x^n = 0 \  .
\end{align*}
We get the recursion relation
$\displaystyle c_{n+2} = \frac{c_n}{(n+1)(n+2)}$
for $n \geq 0$, where $b_0$ and $b_1$ are free.  We have
$\displaystyle c_2 = \frac{c_0}{2} = \frac{c_0}{2!}$,
$\displaystyle c_4 = \frac{c_2}{3\times 4} = \frac{c_0}{2\times 3 \times 4} 
= \frac{c_0}{4!}$,
$\displaystyle c_6 = \frac{c_4}{5\times 6} = \frac{c_0}{2\times 3
\times 4 \times 5 \times 6} = \frac{c_0}{6!}$, and so on.
By induction, we get $\displaystyle c_n = \frac{c_0}{n!}$ for $n$ even.
Moreover,
$\displaystyle c_3 = \frac{c_1}{2\times 3} = \frac{c_1}{3!}$,
$\displaystyle c_5 = \frac{c_3}{4\times 5} = \frac{c_1}{2\times
3\times 4 \times 5} = \frac{c_1}{5!}$,
$\displaystyle c_7 = \frac{c_5}{6\times 7} = \frac{c_1}{2\times
3\times 4 \times 5 \times 6 \times 7} = \frac{c_1}{7!}$, and so on.
By induction, we get
$\displaystyle c_n = \frac{b_1}{n!}$ for $n$ odd.

By selecting two linearly independent pairs $(c_0,c_1)$, we get two
linearly independent solutions.  The choice $(c_0,c_1) = (0,1)$ yields
the solution of the form $y_1$ above and the choice $(c_0,c_1)= (1,0)$
yields the solution of the form $y_2$ above with $k=0$.  We will
however not choose those pairs.  With $c_0=c_1 =1$, we get the solution
\[
y_1 = \sum_{n=0}^\infty \frac{x^{2n}}{(2n)!}
+ \sum_{n=0}^\infty \frac{x^{2n+1}}{(2n+1)!} 
= \sum_{n=0}^\infty \frac{x^{n}}{n!}  = e^x \  ,
\]
and with $c_0 = -c_1 = 1$, we get the solution
\[
y_2 = \sum_{n=0}^\infty \frac{x^{2n}}{(2n)!}
- \sum_{n=0}^\infty \frac{x^{2n+1}}{(2n+1)!} 
= \sum_{n=0}^\infty \frac{(-1)^nx^{n}}{n!}  = e^{-x} \  .
\]
Those are the two solutions normally found using the standard method
to solve second order differential equation with constant
coefficients.  The general solution is
$\displaystyle y(x) = A e^x + B e^{-x}$ for arbitrary constants $A$
and $B$.
\end{egg}

\begin{egg}
Solve the ordinary differential equation
\[
(1-x^2)y''- 2x y' + 2y = 0 \ .
\]

We have an equation of the form
$\displaystyle y'' + P(x)\, y' + Q(x) y = 0$, where
$\displaystyle P(x) = \frac{-2x}{1-x^2} = -2x \sum_{n=0}^\infty x^{2n}$
and
$\displaystyle Q(x) = \frac{2}{1-x^2} = 2 \sum_{n=0}^\infty x^{2n}$
for $|x|<1$.  Thus
$\displaystyle x\,P(x) = -2 \sum_{n=0}^\infty x^{2n+2}$ and
$\displaystyle x^2\,Q(x) = 2\sum_{n=0}^\infty x^{2n+2}$
for $|x|<1$.  We find $\alpha_0 = \beta_0= 0$.  The indicial equation is
$\displaystyle \lambda^2 + (\alpha_0 - 1)\lambda + \beta_0 = \lambda^2
- \lambda  = \lambda (\lambda -1) = 0$.
Its roots are $\displaystyle \lambda_1 = 1$ and $\lambda_2=0$.
As for the previous example, $\lambda_1-\lambda_2$ is an integer and
we are in the situation where we may take $k=0$ in $y_2$ given by
(\ref{spv_FMchoices}) because $b_0$ and $b_1$ are free as we will see.
We seek solutions of the form
$\displaystyle y = \sum_{n=0}^\infty c_n x^n$.  This will give us
two linearly independent solutions.  If we substitute this
expression for $y$ into $\displaystyle (1-x^2)y''- 2x y' + 2y = 0$, we
get
\begin{align*}
0 & = (1-x^2)y''- 2x y' + 2y
= (1-x^2) \sum_{n=2}^\infty n(n-1) c_n
x^{n-2} - 2 x \sum_{n=1}^\infty n c_n x^{n-1}
+ 2 \sum_{n=0}^\infty c_n x^n \\
&= \sum_{n=2}^\infty n(n-1) c_n x^{n-2}
- \sum_{n=2}^\infty n(n-1) c_n x^n -2 \sum_{n=1}^\infty n c_n x^n
+ 2\sum_{n=0}^\infty c_n x^n \\
&= \sum_{n=0}^\infty (n+2)(n+1) c_{n+2} x^n
- \sum_{n=2}^\infty n(n-1) c_n x^n -2 \sum_{n=1}^\infty n c_n x^n
+ 2\sum_{n=0}^\infty c_n x^n \\
&= 2 (c_0+c_2) + 6 c_3 x + \sum_{n=2}^\infty \big( (n+2)(n+1) c_{n+2} 
- (n+2)(n-1) c_n \big) x^n \ .
\end{align*}
We have that $c_0$ and $c_1$ are free, $c_2 = -c_0$, $c_3 =0$ and we
find the recursion relation
\[
c_{n+2}  = \frac{(n+2)(n-1)}{(n+2)(n+1)} c_n = \frac{n-1}{n+1} c_n
\]
for $n\geq 2$.  Hence
$\displaystyle c_5 = \frac{c_3}{2} = 0$,
$\displaystyle c_7 = \frac{2c_5}{3} = 0$, and so on.
Thus $c_n = 0$ for $n>2$ and odd.  Moreover,
$\displaystyle c_4 = \frac{1}{3} c_2 = - \frac{1}{3} c_0$,
$\displaystyle c_6 = \frac{3}{5} c_4 = - \frac{1}{5} c_0$,
$\displaystyle c_8 = \frac{5}{7} c_6 = - \frac{1}{7} c_0$, and so on.
By induction, we find that
$\displaystyle c_n = - \frac{c_0}{n-1}$ for $n>1$ and even.
A first solution is given by $c_1=1$ and $c_0=0$.  We get
$y_1(x) = x$.  A second solution linearly independent of the first one
is given by $c_1=0$ and $c_0=1$.  We get
$\displaystyle y_2 = \sum_{n=0}^\infty \frac{-x^{2n}}{2n-1}$
for $|x|<1$.  The general solution is
$\displaystyle y = C\, x + D\,\sum_{n=0}^\infty \frac{-x^{2n}}{2n-1}$
for $|x|<1$ and arbitrary constants $C$ and $D$.
\end{egg}

\begin{egg}
Solve the ordinary differential equation
\[
xy''+  y' + x y = 0 \ .
\]

We have an equation of the form
$\displaystyle y'' + P(x)\, y' + Q(x) y = 0$, where
$\displaystyle P(x) = \frac{1}{x}$ and
$\displaystyle Q(x) = 1$.
Thus $\displaystyle x P(x) = 1$ and
$\displaystyle x^2 Q(x) = x^2$.  We find
$\alpha_0 = 1$ and $\beta_0= 0$.  The indicial equation is
$\displaystyle \lambda^2 + (\alpha_0 - 1)\lambda + \beta_0 = \lambda^2 = 0$.
There is only one root of algebraic multiplicity $2$; namely, $\lambda=0$.
One solution is of the form
$\displaystyle y_1 = \sum_{n=0}^\infty a_n x^n$ and another solution
linearly independent of the first one is of the form
$\displaystyle y_2 = \ln(x) y_1(x) + \sum_{n=1}^\infty b_n x^n$.

If we substitute $\displaystyle y = \sum_{n=0}^\infty a_n x^n$ into
$\displaystyle xy''+ y' + xy = 0$, we get
\begin{align*}
0 & = xy''+ y' + xy
= \sum_{n=2}^\infty n(n-1) a_n x^{n-1}
+ \sum_{n=1}^\infty n a_n x^{n-1} + \sum_{n=0}^\infty a_n x^{n+1} \\
&= \sum_{n=0}^\infty (n+2)(n+1) a_{n+2} x^{n+1}
+ \sum_{n=-1}^\infty (n+2)a_{n+2} x^{n+1}
+ \sum_{n=0}^\infty a_n x^{n+1} \\
&= a_1 + \sum_{n=0}^\infty \left( (n+2)^2 a_{n+2} +a_n\right) x^{n+1} \  .
\end{align*}
We have that $a_0$ is free and $a_1=0$.  The recursion relation becomes
$\displaystyle a_{n+2} = -\frac{1}{(n+2)^2} a_n$
for $n \geq 0$.  We find that
$\displaystyle a_3 = \frac{-1}{3^2} a_1 = 0$,
$\displaystyle a_5 = \frac{-1}{5^2} a_3 = 0$, and so on.
Thus $a_{2n-1} = 0 $ for $n>0$ by induction.  We also have that
$\displaystyle a_2 = \frac{-a_0}{2^2}$,
$\displaystyle a_4 = -\frac{1}{4^2}a_2 = \frac{a_0}{2^2\times 4^2}$,
$\displaystyle a_6 = -\frac{1}{6^2}a_4
= \frac{-a_0}{2^2\times 4^2\times 6^2}$, and so on.
By induction, we get that
$\displaystyle a_{2n} = (-1)^n \frac{a_0}{(2^n n!)^2}$
for $n>0$.   With $a_0=1$, we get a first solution
\[
y_1 = \sum_{n=0}^\infty (-1)^n \frac{1}{ (2^n n!)^2}x^{2n} \ .
\]

If we substitute
$\displaystyle y = \ln(x)y_1(x) + \sum_{n=1}^\infty b_n x^n$ into
$\displaystyle xy''+ y' + xy = 0$, we get
\begin{align*}
0 & = xy''+ y' + xy
= x\left( \ln(x) y_1''(x) + \frac{2y_1'(x)}{x}- \frac{y_1(x)}{x^2}\right)
+ \sum_{n=2}^\infty n(n-1) b_n x^{n-1}\\
&\qquad + \left( \ln(x) y_1'(x) + \frac{y_1(x)}{x}\right) 
+ \sum_{n=1}^\infty n b_n x^{n-1} + x \ln(x) y_1(x)
+ \sum_{n=1}^\infty b_n x^{n+1} \\
&= \ln(x) \underbrace{\left( x y_1''(x) + y_1'(x) + x y_1(x)\right)}_{=0}
+2 y_1'(x) + \sum_{n=0}^\infty (n+2)(n+1) b_{n+2} x^{n+1} \\
&\qquad + \sum_{n=-1}^\infty (n+2) b_{n+2} x^{n+1}
+ \sum_{n=1}^\infty b_n x^{n+1} \\
&=4\sum_{n=1}^\infty (-1)^n \frac{n}{ (2^n n!)^2}x^{2n-1} + b_1 + 4 b_2x
+ \sum_{n=1}^\infty \big( (n+2)^2 b_{n+2} + b_n\big) x^{n+1} \ .
\end{align*}
If we collect the terms in $x^n$ with $n$ even, and those in $x^n$ with
$n$ odd, we get
\begin{align*}
0 &= b_1 + \sum_{n=1}^\infty \left((2n+1)^2 b_{2n+1}+b_{2n-1}\right) x^{2n}
\end{align*}
and
\begin{align*}
0 &= 4\sum_{n=1}^\infty (-1)^n \frac{n}{ (2^n n!)^2}x^{2n-1} + 4b_2 x 
+ \sum_{n=2}^\infty \big( (2n)^2 b_{2n} + b_{2n-2}\big) x^{2n-1} \\
&= (4b_2-1)x + 4\sum_{n=2}^\infty\left((-1)^n \frac{n}{(2^n n!)^2}
+ (2n)^2 b_{2n} + b_{2n-2}\right) x^{2n-1} \  .
\end{align*}
Thus, $b_1=0$ and
$\displaystyle b_{2n+1} = - \frac{1}{(2n+1)^2}b_{2n-1}$
for $n >0$.  By induction, we get $b_{2n+1} = 0$ for $n \geq 0$.
Moreover, $b_2 = 1/4$ and
$\displaystyle b_{2n} = -\frac{1}{(2n)^2} \left( b_{2n-2} +
 (-1)^n \frac{4n}{(2^n n!)^2}\right)$
for $n>1$.  Hence,
\begin{align*}
b_4 &= -\frac{1}{4^2} \left( \frac{1}{4} + \frac{8}{(2^2 2!)^2}\right)
= -\frac{1}{2^4 (2!)^2}\left( 1 + \frac{1}{2} \right) \\
b_6 &= -\frac{1}{6^2} \left( b_4 - \frac{12}{(2^3 3!)^2}\right)
= -\frac{1}{2^6 (3!)^2} \left( \frac{2^6(3!)^2}{6^2}b_4 -\frac{1}{3}\right) \\
&= -\frac{1}{2^6 (3!)^2} \left( 
-\frac{2^6(3!)^2}{2^4 (2!)^2 6^2}\left( 1 + \frac{1}{2} \right)
-\frac{1}{3}\right)
= \frac{1}{2^6 (3!)^2} \left( 1 + \frac{1}{2} + \frac{1}{3}\right) \ ,
\end{align*}
and so on.  By induction, we find that
$\displaystyle b_{2n} = \frac{(-1)^{n-1}}{2^{2n}(n!)^2}\left(1+\frac{1}{2}
  +\frac{1}{3} + \ldots + \frac{1}{n}\right)$
for $n > 0$.

A second solution linearly independent of $y_1$ is given by
\[
y_2 = \ln(x) y_1(x) + \sum_{n=1}^\infty 
\frac{(-1)^{n-1}}{2^{2n}(n!)^2}\left(1+\frac{1}{2}
  +\frac{1}{3} + \ldots + \frac{1}{n}\right) x^{2n}  \ .
\]

The general solution is
$\displaystyle y = C\, y_1(x) + D\, y_2(x)$ for arbitrary constants
$C$ and $D$.

In fact, we have that $y_1(x) = J_0(x)$, the ``Bessel function of
the first kind of order zero'', and $y_2(x) = Y_0(x)$, the ``Bessel
function of the second kind of order zero''.  We will study these
functions in a future section.
\end{egg}

\begin{egg}
Solve the ordinary differential equation
\[
xy''+  y' + 2 y = 0 \ .
\]
We have an equation of the form
$\displaystyle y'' + P(x)\, y' + Q(x) y = 0$, where
$\displaystyle P(x) = \frac{1}{x}$ and
$\displaystyle Q(x) = \frac{2}{x}$.
Thus $\displaystyle x P(x) = 1$ and
$\displaystyle x^2 Q(x) = 2x$.  We find
$\alpha_0 = 1$ and $\beta_0= 0$.  The indicial equation is
$\displaystyle \lambda^2 + (\alpha_0 - 1)\lambda + \beta_0 = \lambda^2 = 0$.
There is only one root of algebraic multiplicity $2$; namely, $\lambda=0$.

This time, we will not use the second formula in (\ref{spv_FMchoices})
to find two linearly independent solutions.  Instead, we will use the
method involving the derivative with respect to $\lambda$ mentioned at
the beginning of the section.

We seek a solution of the form
$\displaystyle y = \sum_{n=0}^\infty c_n x^{n+\lambda}$.
If we substitute this expression into
$\displaystyle xy''+ y' + 2y = 0$, we get
\begin{align*}
0 & = xy''+ y' + 2y
= \sum_{n=0}^\infty (\lambda +n)(\lambda + n-1) c_n x^{\lambda + n-1}
+ \sum_{n=0}^\infty (\lambda +n) c_n x^{\lambda + n-1}
+ 2\sum_{n=0}^\infty c_n x^{\lambda + n} \\
&= \sum_{n=-1}^\infty (\lambda +n+1)(\lambda +n) c_{n+1} x^{\lambda +n}
+ \sum_{n=-1}^\infty (\lambda +n+1)c_{n+1} x^{\lambda +n}
+ 2\sum_{n=0}^\infty c_n x^{\lambda +n} \\
&= \lambda^2 c_0 x^{\lambda -1}
+ \sum_{n=0}^\infty \left( (\lambda +n+1)^2 c_{n+1} +
2c_n\right) x^{\lambda +n} \ .
\end{align*}
From the recurrence relation
$\displaystyle c_{n+1} = - \frac{2c_n}{(\lambda +n+1)^2}$
for $n \geq 0$, we find by induction that
\[
c_n = \frac{(-2)^n c_0}
{(\lambda +n)^2(\lambda +n-1)^2\cdots (\lambda + 1)^2}
\]
for $n \geq 1$. With these values for $c_n$, the function
\[
y(x) = c_0 x^\lambda + \sum_{n=1}^\infty 
\frac{(-2)^n c_0} {(\lambda +n)^2(\lambda +n-1)^2\cdots (\lambda + 1)^2}
x^{n+\lambda}
\]
satisfies $xy''+ y' + 2y = \lambda^2 c_0 x^{\lambda -1}$.
If $\lambda = 0$, we get the solution
\[
y_1(x) = c_0 + \sum_{n=1}^\infty
\frac{(-2)^n c_0} {n^2 (n-1)^2 \cdots 1^2}x^n
= c_0 \sum_{n=0}^\infty \frac{(-2)^n} {(n!)^2} x^n \  .
\]
Note that we may have $\displaystyle \lambda^2 c_0 = 0$ with $c_0 \neq 0$
because $\lambda =0$; namely, $c_0$ is free.

To get a second solution linearly independent of the first, we
consider
\begin{align*}
y(x) &= c_0 x^\lambda + \sum_{n=1}^\infty 
\frac{(-2)^n c_0} {(\lambda +n)^2(\lambda +n-1)^2\cdots (\lambda + 1)^2}
x^{n+\lambda} \\
&= x^\lambda \left(c_0  + \sum_{n=1}^\infty 
\frac{(-2)^n c_0} {(\lambda +n)^2(\lambda +n-1)^2\cdots (\lambda + 1)^2}
x^n\right) \ .
\end{align*}
This second solution is given by
\begin{align*}
y_2(x) &= \pdydx{y}{\lambda}(x,\lambda)\big|_{\lambda=0}
= \left(x^\lambda\ln(x) \left(c_0  + \sum_{n=1}^\infty 
\frac{(-2)^n c_0} {(\lambda +n)^2(\lambda +n-1)^2\cdots (\lambda + 1)^2}
x^n\right)\right)\bigg|_{\lambda=0} \\
&\qquad + \left(x^\lambda \sum_{n=1}^\infty (-2)^n c_0
\pdfdx{\Big((\lambda +n)^{-2}(\lambda +n-1)^{-2}\cdots
(\lambda + 1)^{-2}\Big)}{\lambda} x^n\right)\bigg|_{\lambda=0} \\
&= \ln(x) y_1(x) + \sum_{n=1}^\infty (-2)^n c_0 \Big(\left(
-2(\lambda +n)^{-1}-2(\lambda +n-1)^{-1} - \ldots -2 (\lambda + 1)^{-1}\right) \\
&\qquad (\lambda +n)^{-2}(\lambda +n-1)^{-2}\cdots (\lambda +
1)^{-2}\Big)\bigg|_{\lambda=0} x^n \\
&= \ln(x) y_1(x) + c_0 \sum_{n=1}^\infty \frac{(-2)^{n+1}}{(n!)^2}
\left(1 + \frac{1}{2} + \frac{1}{3} + \ldots + \frac{1}{n}\right) x^n
\end{align*}

The general solution is
$\displaystyle y = C\, y_1(x) + D\, y_2(x)$ for arbitrary constants
$C$ and $D$.
\end{egg}

\section{Sturm-Liouville Problems}

As we will see later, results from the theory of Sturm-Liouville
problems are used to show that the Legendre polynomials and the Bessel
functions form two complete orthogonal sets of functions in some
spaces of functions.

All linear second order ordinary differential equation can be written as
\[
(r(x) y')' + (q(x) + \lambda p(x)) y = 0 \ .
\]
If we consider this ordinary differential equation on an interval
$]a,b[$ and add some conditions on $y$ or $y'$ at the end points $a$
and $b$, we get a
{\bfseries Sturm-Liouville problem}\index{Sturm-Liouville Problem}.

The {\bfseries regular Sturm Liouville problem}\index{Sturm-Liouville
Problem!Regular} is to solve 
\begin{equation} \label{spv_sturm_liou_equ}
(r(x) y')' + (q(x) + \lambda p(x)) y = 0
\end{equation}
for $-\infty < a < x < b < \infty$ with the boundary conditions
\[
\alpha_1 y(a) + \alpha_2 y'(a) = 0 \quad \text{and} \quad
\beta_1 y'(b) + \beta_2 y(b) = 0 \ ,
\]
where $(\alpha_1,\alpha_1)\neq (0,0)$ and $(\beta_1,\beta_2) \neq (0,0)$;
$r$, $q$, $p$ and $r'$ are continuous real functions on $[a,b]$; and
$p(x)>0$ for all $x \in [a,b]$.

There are three other types of boundary conditions which are often used.
\begin{enumerate}
\item If $r(a)=0$, we request that the solution and its derivative be bounded
at $x=a$ and that $\beta_1 y'(b) + \beta_2 y(b) = 0$ with
$(\beta_1,\beta_2) \neq (0,0)$.
\item If $r(b)=0$, we request that the solution and its derivative be bounded
at $x=b$ and that $\alpha_1 y'(a) + \alpha_2 y(a) = 0$ with
$(\alpha_1,\alpha_2) \neq (0,0)$.
\item If $r(a) = r(b)$, we request the periodic boundary conditions
$y(a) = y(b)$ and $y'(a) = y'(b)$.
\end{enumerate}

The Sturm-Liouville problem is said to be
{\bfseries singular}\index{Sturm-Liouville Problem!Singular} if one
of the following conditions is satisfied:
(1) $a=\infty$ or $b=\infty$, (2) $r$ or $p$ are null at $a$ or $b$, or
(3) $r'$ does not exist at $a$ or $b$.

Sturm-Liouville problems which are {\bfseries singular} only
because $r(a) = 0$ (resp.\ $r(b)=0$) have many of the properties
that regular Sturm-Liouville problems have if we assume that the
solution is bounded at $a$ (resp.\ $b$).  Such problems are called
{\bfseries regular singular Sturm-Liouville problem}%
\index{Sturm-Liouville Problem!Regular Singular}. 

A value of $\lambda$ for which the Sturm-Liouville problem has non-trivial
solutions is called an {\bfseries eigenvalue}%
\index{Sturm-Liouville Problem!Eigenvalue}, the non-trivial solutions are
called the {\bfseries eigenfunctions}%
\index{Sturm-Liouville Problem!Eigenfunctions} associated to the eigenvalue
$\lambda$.  The following result has a simple proof.

\begin{prop}\label{SL_orthog}
The eigenfunctions associated to distinct eigenvalues are orthogonal with
respect to the weight function $p$; more precisely, if $y_n$ is an
eigenfunction associate to $\lambda_n$ and $y_m$ is an eigenfunction
associate to $\lambda_m$ with $n \neq m$, then
\begin{equation} \label{spv_sturm_liou_sp} 
\int_a^b p(x) y_n(x) y_m(x) \dx{x} = 0 \ .
\end{equation}
\end{prop}

\begin{proof}
Let $y_n$ be a solution of (\ref{spv_sturm_liou_equ}) with
$\lambda=\lambda_n$ and $y_m$ be a solution of (\ref{spv_sturm_liou_equ})
with $\lambda=\lambda_n$.  We have
$\displaystyle (r(x) y_n')' + q(x)y_n = - \lambda_n p(x) y_n$ and
$\displaystyle (r(x) y_m')' + q(x)y_m = - \lambda_m p(x) y_m$.
If we multiply the first equation by $y_m$ and the second by $y_n$, and
subtract the resulting second equation from the resulting first
equation, then we get
\[
(r(x) y_n')'y_m - (r(x) y_m')'y_n = (\lambda_m - \lambda_n)p(x) y_n y_m \ .
\]
Since
\[
\dfdx{\big(r(x) y_n'y_m - r(x) y_m'y_n\big)}{x} =
(r(x) y_n')'y_m - (r(x) y_m')'y_n \ ,
\]
we get
\[
\dfdx{\big(r(x) y_n'y_m - r(x) y_m'y_n\big)}{x}
= (\lambda_m - \lambda_n)p(x) y_n y_m \ .
\]
If we integrate both sides with respect to $x$ for $a< x < b$, then we get
\begin{equation} \label{SL_orth_proof}
\begin{split}
(\lambda_m - \lambda_n) \int_a^b p(x) y_n(x) y_m(x) \dx{x} &=
\big(r(x) y_n'(x)y_m(x) - r(x) y_m'(x)y_n(x)\big)\Big|_a^b \\
&= \left( r(x)
\det \begin{pmatrix} y_m(x) & y_m'(x) \\ y_n(x) & y_n'(x) \end{pmatrix}
\right)\bigg|_a^b \ .
\end{split}
\end{equation}

\stage{i} Suppose that $y_n$ and $y_m$ satisfy the boundary conditions
$\displaystyle \alpha_1 y(a) + \alpha_2 y'(a) = 0$
with $(\alpha_1,\alpha_2)\neq (0,0)$ and
$\displaystyle \beta_1 y'(b) + \beta_2 y(b) = 0$ with
$(\beta_1,\beta_2) \neq (0,0)$.

The boundary condition
$\alpha_1 y_n(a) + \alpha_2 y_n'(a) = 0$ and
$\alpha_1 y_m(a) + \alpha_2 y_m'(a) = 0$ with
$(\alpha_1,\alpha_1)\neq (0,0)$ implies that
\[
\begin{pmatrix} y_m(a) & y_m'(a) \\ y_n(a) & y_n'(a) \end{pmatrix}
\begin{pmatrix} u \\ v \end{pmatrix}
= \begin{pmatrix} 0 \\ 0 \end{pmatrix}
\]
has a non-trivial solution; namely $(u,v) = (\alpha_1, \alpha_2)$.  This is
possible (if and) only if\\
$\displaystyle 
\det \begin{pmatrix} y_m(a) & y_m'(a) \\ y_n(a) & y_n'(a) \end{pmatrix} = 0$.
Similarly,
$\displaystyle
\det \begin{pmatrix} y_m(b) & y_m'(b) \\ y_n(b) & y_n'(b) \end{pmatrix} = 0$.

It follows from (\ref{SL_orth_proof}) that
$\displaystyle (\lambda_m - \lambda_n) \int_a^b p(x) y_n(x) y_m(x) \dx{x} = 0$.
Thus,\\
$\displaystyle \int_a^b p(x) y_n(x) y_m(x) \dx{x} = 0$ for
$\lambda_m \neq \lambda_n$.

\stage{ii} Suppose that $r(a)=0$; that $y_n$, $y_m$, $y_n'$ and $y_m'$ are
bounded at $x=a$; and that $y_n$ and $y_m$ satisfy the boundary
condition $\beta_1 y'(b) + \beta_2 y(b) = 0$ with
$(\beta_1,\beta_2) \neq (0,0)$.

The right hand side of (\ref{SL_orth_proof}) is null at $x=a$ because
$r(a)=0$, and $y_n$, $y_m$, $y_n'$, $y_m'$ are bounded at $x=a$.
The boundary condition
$\displaystyle \beta_1 y_n(b) + \beta_2 y_n'(b) = 0$ and
$\displaystyle \beta_1 y_m(b) + \beta_2 y_m'(b) = 0$
with $(\beta_1,\beta_2)\neq (0,0)$ implies that
\[
\begin{pmatrix} y_m(b) & y_m'(b) \\ y_n(b) & y_n'(b) \end{pmatrix}
\begin{pmatrix} u \\ v \end{pmatrix}
= \begin{pmatrix} 0 \\ 0 \end{pmatrix}
\]
has a non-trivial solution; namely, $(u,v) = (\beta_1,\beta_2)$.  This is
possible (if and) only if\\
$\displaystyle
\det \begin{pmatrix} y_m(b) & y_m'(b) \\ y_n(b) & y_n'(b) \end{pmatrix} = 0$.
Therefore, the right hand side of (\ref{SL_orth_proof}) is also null at
$x=b$.

It follows from (\ref{SL_orth_proof}) that
$\displaystyle (\lambda_m - \lambda_n) \int_a^b p(x) y_n(x) y_m(x) \dx{x} = 0$.
Thus,\\
$\displaystyle \int_a^b p(x) y_n(x) y_m(x) \dx{x} = 0$ for
$\lambda_m \neq \lambda_n$.

\stage{iii} Suppose that $r(b)=0$; that $y_n$, $y_m$, $y_n'$ and $y_m'$
are bounded at $x=b$; and that $y_n$ and $y_m$ satisfy the boundary
condition $\alpha_1 y'(a) + \alpha_2 y(a) = 0$ with
$(\alpha_1,\alpha_2) \neq (0,0)$.

The proof that $\displaystyle \int_a^b p(x) y_n(x) y_m(x) \dx{x} = 0$ for
$\lambda_m \neq \lambda_n$ is similar to the proof given in (ii).

\stage{iv} Suppose that $r(a) = r(b)$ and that the periodic
boundary conditions $y(a) = y(b)$ and $y'(a) = y'(b)$ are satisfied by $y_n$
and $y_m$.

It follows that
\[
\big(r(x) y_n'(x)y_m(x) - r(x) y_m'(x)y_n(x)\big)\Big|_a^b = 0 \ .
\]
Hence, (\ref{SL_orth_proof}) yields again
$\displaystyle \int_a^b p(x) y_n(x) y_m(x) \dx{x} = 0$ for
$\lambda_m \neq \lambda_n$.
\end{proof}

For the regular Sturm-Liouville problem, there is an
infinite number of distinct eigenvalues
$\lambda_0 < \lambda_1 < \lambda_2 < \ldots$ converging to $+\infty$.
The subspace of all eigenfunctions associated to an eigenvalue $\lambda_n$
is one-dimensional.  If we select an eigenfunction $\phi_n$ associated
to the eigenvalue $\lambda_n$ for each $n$, then we get a complete
orthogonal set $\displaystyle \left\{\phi_n\right\}_{n=0}^\infty$ in
$\displaystyle L^2[a,b]$ with respect to the scalar product defined by
\[
  \ps{f}{g} = \int_a^b f(x) g(x) p(x) \dx{x}
\]
for $\displaystyle f, g \in L^2[a,b]$.  All these results are proved by first
expressing the Sturm-Liouville problem into an integral equation of
the form
\[
  (T_K y)(x) = \int_a^b K(x,t) y(t) \dx{t} = \frac{1}{\lambda} y(x)
\]
for $\displaystyle y \in L^2[a,b]$, where
$\displaystyle K \in L^2([a,b]\times[a,b])$ is a
Hilbert-Schmidt kernel satisfying $K(x,t) = K(t,x)$ for almost all $x$
and $t$.  The function $K$ is given by a ``Green function''.
Details of this proof can found in \cite{RenRog}.  Because of
this symmetry, we get from Theorem~\ref{fu_an_HSKern} that
$T_K$ is a self-adjoint, compact operator on $\displaystyle L^2[a,b]$.  The
results about $\displaystyle \{\phi_n\}_{n=0}^\infty$ stated above are
consequences of the Hilbert-Schmidt Theorem,
Theorem~\ref{fu_an_HStheorem}.  Note that the eigenvalues of the
operator $T_K$ are of the form $1/\lambda$ where $\lambda$ is what we
have defined as an eigenvalue for the Sturm-Liouville problem.
Mathematics is full of these historical inconsistencies.

\begin{egg}
Find the eigenvalues and eigenfunctions of the following
Sturm-Liouville problem.
\[
y''+ \lambda y = 0 \quad \text{with} \quad y'(0)= y'(5) = 0 \  .
\]

There are three cases to consider.

\stage{i} For $\lambda < 0$, the solutions are of the form
$\displaystyle y = A\, e^{\sqrt{-\lambda}\,x}+
B\, e^{-\sqrt{-\lambda}\,x}$.

\stage{ii} For $\lambda = 0$, the solutions are of the form
$\displaystyle y = A\,x + B$.

\stage{iii} For $\lambda > 0$, the solutions are of the form
$\displaystyle y = A\, \cos\left(\sqrt{\lambda}\,x\right)+
B\, \sin\left(\sqrt{\lambda}\,x\right)$.

\stage{i} For $\lambda < 0$, we have
$\displaystyle y' = A\,\sqrt{-\lambda}\, e^{\sqrt{-\lambda}\,x} -
B\, \sqrt{-\lambda}\, e^{-\sqrt{-\lambda}\,x}$.
The boundary condition $y'(0)=0$ yields
$A\,\sqrt{-\lambda} - B\, \sqrt{-\lambda} = 0$ and, after a division
by $\sqrt{-\lambda}$, we get $A-B=0$.  The boundary condition
$y'(5) = 0$ yields
$\displaystyle A\,\sqrt{-\lambda}\, e^{5\sqrt{-\lambda}} -
B\, \sqrt{-\lambda}\, e^{-5\sqrt{-\lambda}} =0$ and, after a division
by $\sqrt{-\lambda}$, we get
$\displaystyle A\, e^{5\sqrt{-\lambda}} - B\, e^{-5\sqrt{-\lambda}} =0$.
We seek the solutions of the linear system
\[
\begin{pmatrix}
1 & -1 \\
e^{5\sqrt{-\lambda}} & -e^{-5\sqrt{-\lambda}}
\end{pmatrix}
\begin{pmatrix} A\\ B \end{pmatrix}
= \begin{pmatrix} 0 \\ 0 \end{pmatrix} \ .
\]
Since
\[
\det 
\begin{pmatrix}
1 & -1 \\
e^{5\sqrt{-\lambda}} & -e^{-5\sqrt{-\lambda}}
\end{pmatrix}
= e^{5\sqrt{-\lambda}} - e^{-5\sqrt{-\lambda}} 
= \left(e^{10\sqrt{-\lambda}} -1\right) e^{-5\sqrt{-\lambda}}
\neq 0
\]
for $\lambda < 0$, the only solution of the linear system is
$A=B=0$; namely, we get the trivial solution.  Therefore, there is no eigenvalue
and eigenfunction for $\lambda <0$.

\stage{ii} For $\lambda = 0$, we have $\displaystyle y' = A$.
The boundary conditions $y'(0)=0$ and $y'(5) = 0$ give $A=0$.
There is no constrain on $B$.  We get a family of non-trivial
solutions.

We have that $\lambda = \lambda_0 = 0$ is an eigenvalue and an
eigenfunction associated to this eigenvalue is $y_0(x)=1$ for all $x$
if we take $B=1$.

\stage{iii} For $\lambda > 0$, we have
$\displaystyle y' = -A\,\sqrt{\lambda}\, \sin\left(\sqrt{\lambda}\,x\right) +
B\, \sqrt{\lambda}\, \cos\left(\sqrt{\lambda}\,x\right)$.
The boundary condition $y'(0)=0$ gives
$B\, \sqrt{\lambda} = 0$.  Thus $B=0$.  The boundary condition
$y'(5) = 0$ gives
$\displaystyle -A\,\sqrt{\lambda}\,\sin\left(5\sqrt{\lambda}\right)=0$,
where we have used $B=0$.  After a division by $-\sqrt{\lambda}$, we
get $\displaystyle A\, \sin\left( 5\sqrt{\lambda}\right) =0$.
Since we are looking for non-trivial solutions, we must assume that
$A\neq 0$.  We obtain
$\displaystyle \sin\left( 5\sqrt{\lambda}\right) = 0$.  Thus,
$\displaystyle 5\sqrt{\lambda} = n\pi$ for $n \geq 1$.

We have found the eigenvalues
$\displaystyle \lambda = \lambda_n = \left(n\pi / 5\right)^2$
for $n \geq 1$.  The eigenfunctions associated to the 
eigenvalue $\lambda_n$ are real multiples of
$\displaystyle y_n = \cos\left( n\pi x / 5 \right)$.

We have that $y''+ \lambda y = 0$ is of the form
(\ref{spv_sturm_liou_equ}) with $r(x) = q(x) = p(x) = 1$ for all $x$.
Thus, from Proposition~\ref{SL_orthog}, we get
\[
\int_0^5 y_n(x) y_m(x) \dx{x} = 0
\]
for $n \neq m$ as it can easily be verified directly.  Moreover,
$\displaystyle \left\{ \cos(n\pi x/5\right\}_{n=0}^\infty$ is a
complete orthogonal set in $\displaystyle L^2[0,5]$.  This is something that we
have used when studying Fourier cosine series.
\end{egg}

\begin{egg}
Find the eigenvalues and eigenfunctions of the following
Sturm-Liouville problem.
\[
y''+ \lambda y = 0 \quad \text{with} \quad y'(0)=0 \quad \text{and}
\quad y(L) = 0 \ .
\]

As in the previous example, there are three cases to consider.

\stage{i} For $\lambda < 0$, the solutions are of the form
$\displaystyle y = A\, e^{\sqrt{-\lambda}\,x}+
B\, e^{-\sqrt{-\lambda}\,x}$.

\stage{ii} For $\lambda = 0$, the solutions are of the form
$\displaystyle y = A\,x + B$.

\stage{iii} For $\lambda > 0$, the solutions are of the form
$\displaystyle y = A\, \cos\left(\sqrt{\lambda}\,x\right)+
B\, \sin\left(\sqrt{\lambda}\,x\right)$.

\stage{i} For $\lambda < 0$, we have
$\displaystyle y' = A\,\sqrt{-\lambda}\, e^{\sqrt{-\lambda}\,x} -
B\, \sqrt{-\lambda}\, e^{-\sqrt{-\lambda}\,x}$.
The boundary condition $y'(0)=0$ yields
$A\,\sqrt{-\lambda} - B\, \sqrt{-\lambda} = 0$ and, after a division
by $\sqrt{-\lambda}$, we get $A-B=0$.  The boundary condition
$y(L) = 0$ yields
$\displaystyle A\, e^{L\sqrt{-\lambda}} + B\, e^{-L\sqrt{-\lambda}} =0$.

We seek the solutions of the linear system
\[
\begin{pmatrix}
1 & -1 \\
e^{L\sqrt{-\lambda}} & e^{-L\sqrt{-\lambda}}
\end{pmatrix}
\begin{pmatrix} A\\ B \end{pmatrix}
= \begin{pmatrix} 0 \\ 0 \end{pmatrix} \  .
\]
Since
\[
\det 
\begin{pmatrix}
1 & -1 \\
e^{L\sqrt{-\lambda}} & e^{-L\sqrt{-\lambda}}
\end{pmatrix}
= e^{-L\sqrt{-\lambda}} + e^{L\sqrt{-\lambda}} 
> 0
\]
for $\lambda < 0$, the only solution of the linear system is $A=B=0$;
namely, we get the trivial solution.  There is no eigenvalue and
eigenfunction for $\lambda <0$.

\stage{ii} For $\lambda = 0$, we have $\displaystyle y' = A$.
The boundary conditions $y'(0)= A= 0$ and $y(L) = AL +B = 0$ yields
$A=B=0$.  Again, there is no non-trivial solution.  Hence, $0$ is
not an eigenvalue.

\stage{iii} For $\lambda > 0$, we have
$\displaystyle y' = -A\,\sqrt{\lambda}\, \sin\left(\sqrt{\lambda}\,x\right) +
B\, \sqrt{\lambda}\, \cos\left(\sqrt{\lambda}\,x\right)$.
The boundary condition $y'(0)=0$ gives
$B\, \sqrt{\lambda} = 0$.  Thus $B=0$.  The boundary condition
$y(L) = 0$ gives
$\displaystyle A\,\cos\left(L\sqrt{\lambda}\right)=0$,
where we have used $B=0$. Since we are looking for non-trivial
solutions, we must assume that $A\neq 0$.  Hence
$\displaystyle \cos\left( L\sqrt{\lambda}\right) = 0$.  Thus,
$\displaystyle L\sqrt{\lambda} = n\pi + \pi/2$ for $n \geq 0$.

We have found the eigenvalues
$\displaystyle \lambda = \lambda_n = \big( (2n+1)\pi/(2L) \big)^2$
for $n \geq 0$.  The eigenfunctions associated to the
eigenvalue $\lambda_n$ are of the form
$\displaystyle y = A \, \cos\big((2n+1)\pi x/ (2L) \big)$
for $n \geq 0$, where $A$ is a non-zero constant.
\end{egg}

\section{Legendre Polynomials $P_n(x)$ on $[-1,1]$}
\label{SectLegendrePoly}

Good references on this subject and the subjects of the next sections
of this chapter are \cite{Rain,Simm}.  In particular, the reader will
find in these references the proofs of the results that we give
without proofs.

The {\bfseries Legendre differential equation}\index{Legendre
Differential Equation} is
\begin{equation} \label{spv_leg_equ}
(1-x^2)y'' -2xy' + n(n+1) y = 0 \ .
\end{equation}
Using the Frobenius method at the regular point $0$, we find that the
bounded solutions (on the interval $]-1,1[$) of this differential
equation are multiples of
\[
P_n(x) = \frac{1}{2^n} \sum_{m=0}^{\intpt{n/2}} \, (-1)^m \binom{n}{m}
\binom{2n-2m}{n} x^{n-2m} \ ,
\]
where $\intpt{n/2}$ is the greater integer less than or equal to $n/2$.
The polynomials $P_n$ are called the
{\bfseries standardized Legendre polynomials}\index{Legendre
Polynomials!Standardized} because they satisfy the standardization
condition $P_n(1) = 1$.  Moreover, we have that $|P_n(x)| \leq 1$ for
$-1 \leq x \leq 1$.

The polynomials $P_n$ can be generated by the recurrence relation
\[
(n+1) P_{n+1}(x) = (2n+1) x P_n(x) - n P_{n-1}(x)
\]
or by the {\bfseries Rodrigue's formula}\index{Legendre
Polynomials!Rodrigue's Formula} given in the following proposition.

\begin{prop}
\[
P_n(x) = \frac{(-1)^n}{2^n n!} \dfdxn{(1-x^2)^n}{x}{n} \ .
\]
\end{prop}

\begin{proof}
Since
\[
(1-x^2)^n = \sum_{k=0}^n \frac{n!}{(n-k)!k!} (-x^2)^{n-k} \ ,
\]
we have
\begin{align*}
\frac{(-1)^n}{2^n n!} \dfdxn{(1-x^2)^n}{x}{n}
&= \frac{(-1)^n}{2^n n!}
\dfdxn{ \left( \sum_{k=0}^n \frac{n!}{(n-k)!k!} (-x^2)^{n-k} \right)}{x}{n} \\
&= \frac{1}{2^n}\sum_{k=0}^{\intpt{n/2}} \frac{(-1)^k}{(n-k)!k!}
\left( \dfdxn{x^{2n-2k}}{x}{n} \right)
\end{align*}
because the derivative of order $n$ of $x^{2n-2k}$ is zero for 
$2n-2k<n$; namely, $k > n/2$.  Since
\[
\dfdxn{\left(x^{2n-2k}\right)}{x}{n} =
(2n-2k)(2n-2k-1) \cdots (n-2k+1) x^{n-2k}
= \frac{(2n-2k)!}{(n-2k)!} x^{n-2k} \ ,
\]
we get
\begin{align*}
\frac{(-1)^n}{2^n n!} \dfdxn{(1-x^2)^n}{x}{n}
&= \frac{1}{2^n}\sum_{k=0}^{\intpt{n/2}} \frac{(-1)^k (2n-2k)!}{(n-k)!k!(n-2k)!}
x^{n-2k} \\
&= \frac{1}{2^n}\sum_{k=0}^{\intpt{n/2}} (-1)^k
\left(\frac{n!}{(n-k)!k!}\right)
\left(\frac{(2n-2k)!}{(n-2k)!n!}\right) x^{n-2k} \\
&= \frac{1}{2^n} \sum_{k=0}^{\intpt{n/2}} \, (-1)^k \binom{n}{k}
\binom{2n-2k}{n} x^{n-2k} = P_n(x) \ .  \qedhere
\end{align*}
\end{proof}

There is also a generating series for the Legendre polynomials;
namely,
\[
\frac{1}{\sqrt{1-2xt +t^2}} = \sum_{n=0}^{\infty} \, P_n(x) t^n \quad ,
\quad -1 < x,t < 1 \ .
\]
The first six Legendre polynomials are
$\displaystyle P_0(x) = 1$, $\displaystyle P_1(x) = x$,
$\displaystyle P_2(x) = \frac{1}{2}(3x^2-1)$,
$\displaystyle P_3(x) = \frac{1}{2} (5x^3 - 3 x)$,
$\displaystyle P_4(x) = \frac{1}{8}(35x^4 - 30x^2 + 3)$ and
$\displaystyle P_5(x) = \frac{1}{8} (63x^5 -70x^3 + 15x)$.

The Legendre equation (\ref{spv_leg_equ}) can be written as a regular
singular Sturm-Liouville problem.  More precisely, (\ref{spv_leg_equ}) is 
\[
\dfdx{ \left( (1-x^2) \dydx{y}{x}\right)}{x} + \lambda y = 0 \ .
\]
This is (\ref{spv_sturm_liou_equ}) with
$\displaystyle r(x) = 1 -x^2$, $q(x)=0$,
$p(x) =1$, $a=-1$ and $b=1$.  If we require that $y$ be bounded on the
interval $]-1,1[$, we get a regular singular Sturm-Liouville problem
whose eigenvalues are $\lambda_n = n(n+1)$ for $n \geq 0$, and where
the Legendre polynomial $P_n$ is an eigenfunction associated to the
eigenvalue $\lambda_n$.

It follows from the theory for the Sturm-Liouville problem that the
Legendre Polynomials are orthogonal with respect to the scalar product
\[
\ps{f}{g} = \int_{-1}^1 f(x)g(x) \dx{x}
\]
and the set $\left\{ P_n : n \geq 0\right\}$ is complete in
$\displaystyle L^2[-1,1]$.  More precisely, the Legendre polynomials
satisfy the orthogonality relations
\[
\int_{-1}^{1} \: P_n(x) P_m(x) \dx{x} =
\begin{cases}
0 & \mbox{ if } n \neq n \\
\displaystyle \frac{2}{2n+1} & \mbox{ if } n = m
\end{cases}
\]
We also have the following result.

\begin{prop}
The $\displaystyle L^2$-norm of $P_n$ is
\[
\parallel P_n \parallel = \left( \int_{-1}^{1} (P_n(x))^2 \dx{x}
\right)^{1/2} = \left( \frac{2}{2n+1} \right)^{1/2} \ .  \qedhere
\]
\end{prop}

\begin{proof}
Let $\displaystyle I = \| P_n\|^2$.  Using Rodrique's formula, we have
\[
I = \frac{(-1)^n}{2^n n!} \int_{-1}^1 P_n(x) \dfdxn{(1-x^2)^n}{x}{n} \dx{x} \ .
\]
Using integration by parts with $f(x) = P_n(x)$ and
$\displaystyle g'(x) = \dfdxn{(1-x^2)^n}{x}{n}$, so
$f'(x) = P_n'(x)$ and
$\displaystyle g(x) = \dfdxn{(1-x^2)^n}{x}{n-1}$, we get
\begin{align*}
I &= \frac{(-1)^n}{2^n n!} \left( P_n(x) \dfdxn{(1-x^2)^n}{x}{n-1}\bigg|_{-1}^1 
- \int_{-1}^1 P_n'(x) \dfdxn{(1-x^2)^n}{x}{n-1} \dx{x} \right) \\
&= \frac{(-1)^{n+1}}{2^n n!}
 \int_{-1}^1 P_n'(x) \dfdxn{(1-x^2)^n}{x}{n-1} \dx{x}
\end{align*}
because $\displaystyle \dfdxn{(1-x^2)^n}{x}{k} = 0$ at $x=1$ and
$x=-1$ for $k<n$.  Using integration by parts as above, the reader
can easily show by induction that
\[
I = \frac{1}{2^n n!} \int_{-1}^1 P_n^{(n)}(x) (1-x^2) \dx{x} \ .
\]
Since
\[
P_n^{(n)}(x) = \dfdxn{
\left( \frac{(-1)^n}{2^n n!} \dfdxn{(1-x^2)^n}{x}{n}\right)}{x}{n}
= \frac{1}{2^n n!} \dfdxn{(x^2-1)^n}{x}{2n} = \frac{(2n)!}{2^n n!} \ ,
\]
we get
\[
I = \frac{(2n)!}{(2^n n!)^2} \int_{-1}^1 (1-x^2) \dx{x}
= \frac{2(2n)!}{(2^n n!)^2} \int_0^1 (1-x^2) \dx{x} \ .
\]
To compute the integral $\displaystyle \int_0^1 (1-x^2)^n \dx{x}$, we
use the trigonometric substitution $x=\sin(t)$ for
$0 \leq t \leq \pi/2$.
\[
\int_0^1 (1-x^2)^n \dx{x} = \int_0^{\pi/2} \cos^{2n+1}(t) \dx{t} \ .
\]
Using integration by parts with $f(t) = \cos^{2n}(t)$ and
$g'(t) = \cos(t)$, so\\
$f'(t) = -2n \cos^{2n-1}(t)\sin(t)$ and
$g(t) = \sin(t)$, we get
\begin{align*}
\int_0^{\pi/2} \cos^{2n+1}(t) \dx{t} &= \cos^{2n}(t)\sin(t)\bigg|_0^{\pi/2}
+ 2n \int_0^{\pi/2} \cos^{2n-1}\sin^2(t)\dx{t} \\
&= 2n \int_0^{\pi/2} \cos^{2n-1}(t) \dx{t} - 2n \int_0^{\pi/2}
\cos^{2n+1}(t) \dx{t} \ .
\end{align*}
Thus,
\[
\int_0^1 (1-x^2)^n \dx{x} = \int_0^{\pi/2} \cos^{2n+1}(t) \dx{t}
= \frac{2n}{2n+1} \int_0^{\pi/2} \cos^{2n-1}\dx{t} \ .
\]
Repeating this procedure $n-1$ times yields
\[
\int_0^1 (1-x^2)^n \dx{x}
= \frac{(2n)(2n-2)\cdots 2}{(2n+1)(2n-1)\cdots 3}
\underbrace{\int_0^{\pi/2} \cos\dx{t}}_{=1} = \frac{(2^n n!)^2}{(2n+1)!} \ .
\]
Finally, we have that
\[
I = \frac{2(2n)!}{(2^n n!)^2} \left( \frac{(2^n n!)^2}{(2n+1)!} \right)
= \frac{2}{2n+1} \ .    \qedhere
\]
\end{proof}

The Fourier-Legendre expansion of $f$ on $[-1,1]$ is
\begin{equation} \label{SF_FL1}
f = \sum_{n=0}^{\infty}  a_n  P_n
\end{equation}
where
$\displaystyle a_n  =  \frac{2n+1}{2} \int_{-1}^{1} f(x) P_n(x)\dx{x}$
for $n \geq 0$.  The series in (\ref{SF_FL1}) converges in
$\displaystyle L^2[-1,1]$.

With the substitution $x=\cos(\phi)$ for
$0\leq \phi \leq \pi$, we get the complete orthogonal set of functions
$\displaystyle \left\{P_n(\cos(\phi)) : n \geq 0 \right\}$
in $\displaystyle L^2[0.\pi]$, where the scalar product is given by
\[
\ps{f}{g} = \int_0^\pi f(\phi) g(\phi) \sin(\phi) \dx{\phi} \ .
\]
The Fourier-Legendre expansion of $f$ on $[0,\pi]$ with respect to
this complete orthogonal set of functions is
\begin{equation} \label{SF_FL2}
  \tilde{f} \equiv f \circ cos = \sum_{n=0}^{\infty} a_n P_n \circ \cos
\end{equation}
where
\[
a_n = \frac{2n+1}{2} \int_{0}^{\pi} \tilde{f}(\phi) P_n(\cos(\phi))
\sin(\phi) \dx{\phi}
= \frac{2n+1}{2} \int_{-1}^{1} f(x)\, P_n(x) \dx{x}
\]
for $n \geq 0$.  The series in (\ref{SF_FL2}) converges in
$\displaystyle L^2[0,\pi]$.

\begin{egg}
Express in terms of the Legendre polynomials the function
$f(x) = |x|$ for $-1\leq x \leq 1$.

The coefficient are given by
$\displaystyle a_n  =  \frac{2n+1}{2} \int_{-1}^{1} |x| P_n(x)\dx{x}$
for $n \geq 0$.  Since $P_n(x)$ is an odd function when $n$ is odd, we
have that $|x| P_n(x)$ is odd when $n$ is odd and so $a_n =0$ for $n$
odd.  It is a little bit harder to find a general formulae for the
coefficients $a_n$ with $n$ even.  We have
\begin{align*}
a_0 &= \frac{1}{2} \int_{-1}^1 |x| \dx{x} = \int_0^1 x \dx{x} =
\frac{1}{2} \ ,\\
a_2 &= \frac{5}{2} \int_{-1}^1 |x| \frac{3x^2-1}{2} \dx{x} 
= \frac{5}{2} \int_0^1 (3x^3-x) \dx{x} = \frac{5}{8} \ , \\
a_4 &= \frac{9}{2} \int_{-1}^1 |x| \frac{35x^4-30x^2+3}{8} \dx{x}
= \frac{9}{8} \int_0^1 (35x^5-30x^3+3x) \dx{x} = -\frac{3}{16} \ ,
\end{align*}
and so on.  If we use Rodrigue's formula, we find that
\[
a_{2m} = \frac{(-1)^{2m}(4m+1)}{2^{2m+1} (2m)!}
\int_{-1}^{1} |x| \dfdxn{(1-x^2)^{2m}}{x}{2m} \dx{x}
= \frac{(4m+1)}{2^{2m} (2m)!} \int_0^{1} x \dfdxn{(1-x^2)^{2m}}{x}{2m} \dx{x}
\]
for $m \ge 0$.  If we assume that $m>0$, we may use integration by
parts with $f(x) = x$ and
$\displaystyle g'(x) = \dfdxn{(1-x^2)^{2m}}{x}{2m}$, so 
$f'(x) = 1$ and $\displaystyle g(x) = \dfdxn{(1-x^2)^{2m}}{x}{2m-1}$, to
get
\begin{align*}
a_{2m} &= \frac{(4m+1)}{2^{2m} (2m)!}\left(
x \dfdxn{(1-x^2)^{2m}}{x}{2m-1}\bigg|_0^1 
- \int_0^{1} \dfdxn{(1-x^2)^{2m}}{x}{2m-1} \dx{x}\right)\\
&= - \frac{(4m+1)}{2^{2m} (2m)!} \dfdxn{(1-x^2)^{2m}}{x}{2m-2}\bigg|_0^1
= \frac{(4m+1)}{2^{2m} (2m)!} \dfdxn{(1-x^2)^{2m}}{x}{2m-2}\bigg|_{x=0} \ ,
\end{align*}
where the second and third equality come from
$\displaystyle \dfdxn{(1-x^2)^{2m}}{x}{k}\Big|_{x=1} = 0$ for $k<2m$.
The value of $\displaystyle \dfdxn{(1-x^2)^{2m}}{x}{2m-2}$ at $x=0$ is
given by $(2m-2)!$ times the coefficient of $x^{2m-2}$ in
\[
(1-x^2)^{2m} = \sum_{k=0}^{2m} \frac{(2m)!}{(2m-k)!k!} (-x^2)^k \ .
\]
This coefficient is associated to $k$ such that $2k = 2m-2$; namely,
$k=m-1$.  Therefore
\[
a_{2m} = \frac{(4m+1)}{2^{2m} (2m)!}\left(
 \frac{(-1)^{m-1}  (2m)!}{(m+1)!(m-1)!} \right) (2m-2)!
= \frac{(-1)^{m-1} (4m+1) (2m-2)!}{2^{2m}(m+1)!(m-1)!}
\]
for $m>0$.  Hence
\[
|x| = \frac{1}{2}P_0(x) + \frac{5}{8}P_2(x) - \frac{3}{16}P_4(x) + \ldots
= \frac{1}{2}P_0(x) + \sum_{m=1}^\infty
\frac{(-1)^{m-1} (4m+1) (2m-2)!}{2^{2m}(m+1)!(m-1)!} P_{2m}(x) \ .
\]
\end{egg}

\section{Associated Legendre Polynomials
$\displaystyle P_n^m(x)$ on $]-1,1[$}

The {\bfseries associated Legendre equation}\index{Associated Legendre
Equation} is the ordinary differential equation
\begin{equation} \label{spv_ass_leg_equ}
(1-x^2) \dfdx{ \left( (1-x^2) \dydx{y}{x} \right) }{x} +
\left( (1-x^2)\lambda - m^2 \right) y = 0
\end{equation}
for $-1 < x < 1$.  If we differentiate $m$ times with respect to $x$
the Legendre equation
\begin{equation} \label{SLformLegEqu}
\dfdx{ \left( (1-x^2) \dydx{z}{x}\right)}{x} + \lambda z = 0
\end{equation}
for $-1<x<1$, we get the differential equation
\[
(1-x^2) \dydxn{z}{x}{m+2} - 2(m+1)x \dydxn{z}{x}{m+1} +
\left( \lambda - m(m+1)\right) \dydxn{z}{x}{m} = 0
\]
for $-1<x<1$.  If we substitute
$\displaystyle y = (1-x^2)^{m/2}\dydxn{z}{x}{m}$,
or equivalently $\displaystyle \dydxn{z}{x}{m} = (1-x^2)^{-m/2} y$,
in the previous equation and multiply by
$\displaystyle (1-x^2)^{-m/2 +1}$, we get
(\ref{spv_ass_leg_equ}).  Solutions of (\ref{spv_ass_leg_equ}) are
therefore of the form $\displaystyle y = (1-x^2)^{m/2}\dydxn{z}{x}{m}$,
where $z$ is a solution of the Legendre equation (\ref{SLformLegEqu}).

It can be shown that (\ref{spv_ass_leg_equ}) has non-trivial bounded
solution if and only if $\lambda = n(n+1)$ for $n\geq 0$. 
Moreover, when $\lambda = n(n+1)$ for some $n$ fixed, the bounded solutions
of (\ref{spv_ass_leg_equ}) are linear combinations of the
{\bfseries associated Legendre polynomials}\index{Associated Legendre
Polynomials} defined by 
\[
P_n^m(x) = (1-x^2)^{m/2}\, \dydxn{P_n}{x}{m}(x)
\]
for $0 \leq m \leq n$.  Note that $m$ cannot be greater than $n$
because $P_n$ is a polynomial of degree $n$ and thus its derivative of
order greater than $n$ is null.

The associated Legendre polynomials will be useful in the study of
Laplace equation in Chapter~\ref{ChapLaplace}.

\section{Gamma Function}

The Gamma function is needed to define the Bessel functions in the next
section.  The use of the Gamma function in mathematics is not
limited to the definition of the Bessel functions.  In fact, the Gamma
function is present in many fields of mathematics.  The Gamma function
plays an important role in mathematical analysis, in analytic number
theory, etc.

The {\bfseries gamma function}\index{Gamma Function} $\Gamma$ is defined by
\[
\Gamma(\nu) = \int_0^\infty e^{-t} t^{\nu-1} \dx{t}
\]
for $\nu > 0$ and recursively by
$\displaystyle \Gamma(\nu) = \Gamma(\nu + 1)/\nu$
for $\nu < 0$, $\nu$ not a negative integer.  The graph of the gamma
function between $-3$ and $3$ is given in Figure~\ref{gamma_fig}.

Some of the properties of the gamma function are
$\displaystyle \Gamma(\nu+1) = \nu \Gamma(\nu)$ and
$\displaystyle \Gamma(n+1) = n!$ for $n \geq 0$.

\pdfF{special_functions/gamma}{Graph of the gamma function}{The
graph of the gamma function $y = \Gamma(\nu)$}{gamma_fig}

We note that $\Gamma(1/2) = \sqrt{\pi}$.  This can easily be proved
with a double integral.  With the substitution $t = s^2$ in
\[
\Gamma\left(\frac{1}{2}\right) = \int_0^\infty e^{-t} t^{-1/2} \dx{t} \ ,
\]
we get
\[
\Gamma\left(\frac{1}{2}\right) = 2 \int_0^\infty e^{-s^2} \dx{s} \ .
\]
Hence,
\begin{align*}
\Gamma^2\left(\frac{1}{2}\right)
&= 4 \left(\int_0^\infty e^{-u^2} \dx{u}\right)^2
= 4 \left(\int_0^\infty e^{-u^2} \dx{u}\right)
\left( \int_0^\infty e^{-v^2} \dx{v}\right) \\
&= 4 \int_0^\infty \int_0^\infty e^{-u^2-v^2} \dx{u} \dx{v}
= 4 \int_0^{\pi/2} \int_0^\infty r e^{-r^2} \dx{r}\dx{\theta} = \pi \ ,
\end{align*}
where we have used the polar coordinates $u=r\cos(\theta)$,
$v=r\sin(\theta)$ for $0\leq \theta \leq \pi/2$ and $r\geq 0$ to
compute the double integral.

\section{Bessel Functions}

The {\bfseries Bessel differential equation}\index{Bessel Differential
Equation} is 
\begin{equation} \label{spv_bess_equ}
x^2 y'' + x y' + (x^2 - \nu^2) y = 0 \ .
\end{equation}
Using the Frobenius method at the regular singular point $0$, we find
the following solutions.  If $\nu$ is not and integer, the functions
\[
J_\nu(x) = \sum_{k=0}^{\infty} \, \frac{(-1)^k}{k! \Gamma(\nu+k+1)}
\left(\frac{x}{2}\right)^{\nu+2k}
\]
and $J_{-\nu}$ are two independent solutions of the Bessel equation.
The functions $J_\nu$ are the {\bfseries Bessel functions of the first
kind}\index{Bessel Functions!Of the First Kind}.

If $\nu = n$ is an integer,
\[
J_n(x)
= \frac{x^n}{2^n n!} \left( 1 - \frac{x^2}{2^2 1! (n+1)} +
\frac{x^4}{2^4 2! (n+1)(n+2)} - \frac{x^6}{2^6 3! (n+1)(n+2)(n+3)} +
\dots \right)
\]
and $J_{-n}(x) = (-1)^n J_n(x)$.  So, $J_n$ and $J_{-n}$ are not
linearly independent.

Let
\[
Y_\nu(x) = \lim_{\rho \rightarrow \nu}
\frac{J_{\rho}(x) \cos(\rho \pi) - J_{-\rho}(x)}{\sin(\rho \pi)} \  .
\]
When $\nu$ is not an integer, we have
\[
Y_\nu(x) = \frac{J_{\nu}(x) \cos(\nu \pi) - J_{-\nu}(x)}{\sin(\nu \pi)} \  .
\]
When $\nu$ is an integer, $Y_\nu(x)$ can be computed using the
Hospital rule.  The functions $Y_\nu$ are the {\bfseries Bessel
functions of the second kind}\index{Bessel Functions!Of the Second Kind}.

If $\nu = n$ is an integer, a second solution of the Bessel
differential equation, linearly independent of $J_n$, is given 
by $Y_n(x)$.  Note that $Y_\nu$ is a solution of the Bessel
differential equation, linearly independent of $J_\nu(x)$, even when
$\nu$ is not an integer.

\begin{egg}
Find the general solution of the ordinary differential equation
\begin{equation}\label{BesselEX1}
x^2 y'' + xy' + (\lambda^2 x^2 - \nu^2)y = 0
\end{equation}
by reducing it to a Bessel equation.

Let $z=\lambda x$ and
$\displaystyle \tilde{y}(z) = y(x) = y\left(\frac{z}{\lambda}\right)$,
then
\[
\dydx{y}{x} = \dydx{\tilde{y}}{z}\,\dydx{z}{x}
= \lambda\, \dydx{\tilde{y}}{z} \quad \text{and} \quad
\dydxn{y}{x}{2} = \dfdx{\left( \dydx{y}{x} \right)}{x}
= \dfdx{\left( \lambda\,\dydx{\tilde{y}}{z} \right)}{z} \dydx{z}{x}
= \lambda^2 \dydxn{\tilde{y}}{z}{2} \ .
\]
To stick with the tradition, we will use $y$ to denote $\tilde{y}$.
The context will determine if we are working with $y(x)$ or
$\tilde{y}(z)$.  Hence, (\ref{BesselEX1}) becomes
\[
0 = \left(\frac{z}{\lambda}\right)^2 \left( \lambda^2 \dydxn{y}{z}{2}\right)
+ \frac{z}{\lambda} \left(\lambda \dydx{y}{z}\right)
+\left( \lambda^2 \left(\frac{z}{\lambda}\right)^2 - \nu^2\right)
= z^2 \dydxn{y}{z}{2} + z \dydx{y}{z} + (z^2 - \nu^2) y \ .
\]
This is a Bessel equation with the general solution
$y(z) = A J_\nu(z) + B\,Y_\nu(z)$.  Hence,
the general solution of (\ref{BesselEX1}) is
$\displaystyle y(x) = A J_\nu(\lambda x) + B\,Y_\nu(\lambda x)$
for arbitrary constants $A$ and $B$.
\end{egg}

\begin{egg}
Use the substitution $y(x)=\sqrt{x}\, u(x)$ and $z=3x$ to solve
(i.e.\ find the general solution of) the ordinary differential equation
\begin{equation}\label{BesselEX2}
x^2 y''+ \left( 9x^2 + \frac{5}{36}\right) y = 0 \ .
\end{equation}

Since
$\displaystyle y'= u' x^{1/2}+ u  x^{-1/2}/2$ and
$\displaystyle y''= u'' x^{1/2}+ u' x^{-1/2} - u x^{-3/2}/4$,
(\ref{BesselEX2}) becomes
\[
0 =x^2 \left(u'' x^{1/2}+ u' x^{-1/2} - \frac{1}{4} u x^{-3/2}\right)
+\left(9x^2 + \frac{5}{36}\right) u x^{1/2}
= x^{5/2} u'' + x^{3/2} u' + \left( 9  x^2 - \frac{1}{9}\right) u x^{1/2} \ .
\]
Hence, after multiplying both sides of the previous equality by
$\displaystyle x^{1/2}$, we get
\[
x^2 u'' + x u' + \left( 9  x^2 - \frac{1}{9}\right) u =0 \ .
\]
This is almost a Bessel equation but for the coefficient $9$ in front
of $\displaystyle x^2$ in the parenthesis.  Let $z=3x$,  We have
\[
\dydx{u}{x} = \dydx{u}{z}\,\dydx{z}{x} = 3 \dydx{u}{z}
\quad \text{and} \quad
\dydxn{u}{x}{2} = \dfdx{\left( \dydx{u}{x} \right)}{x}
= \dfdx{\left( 3 \dydx{u}{z} \right)}{z} \dydx{z}{x} = 9 \dydxn{u}{z}{2} \ .
\]
Hence,
\begin{align*}
x^2 u'' + x u' + \left( 9 x^2 - \frac{1}{9}\right) u
&= 9 \left(\frac{z}{3}\right)^2\dydxn{u}{z}{2}
+ 3\left( \frac{z}{3}\right) \dydx{u}{z}
+ \left( 9 \left( \frac{z}{3}\right)^2 - \frac{1}{9}\right) u \\
&= z^2  \dydxn{u}{z}{2}
+ z \dydx{u}{z} + \left(z^2 - \frac{1}{9}\right) u =0 \ .
\end{align*}
This is the Bessel equation with $\nu = 1/3$.
The general solution of this Bessel equation is
$\displaystyle u(z) = A J_{1/3}(z) + B J_{-1/3}(z)$.  Therefore, the
general solution of (\ref{BesselEX2}) is
$\displaystyle y(x) = \sqrt{x} \, u(x) =
A \sqrt{x}\, J_{1/3}(3x)+ B \sqrt{x}\, J_{-1/3}(3x)$
for arbitrary constants $A$ and $B$.
\end{egg}

\begin{egg}
Find the general solution of the ordinary differential equation
\begin{equation}\label{BesselEX3}
y''+ k^2 x^2 y = 0
\end{equation}
for $x >0$.

We use the substitution $y(x)=u(x) \sqrt{x}$ and $\displaystyle z= kx^2/2$.
Since $\displaystyle y'= u' x^{1/2}+ u x^{-1/2}/2$
and
$\displaystyle y''= u'' x^{1/2}+ u' x^{-1/2} - u x^{-3/2}/4$,
we get
\[
0 = \left(u'' x^{1/2}+ u' x^{-1/2} - \frac{1}{4} u x^{-3/2}\right)
+ k^2 x^2 \left(u x^{1/2}\right)
= x^{1/2} u'' + x^{-1/2} u' + \left( k^2 x^{5/2} - \frac{1}{4} x^{-3/2}\right) u
\]
from (\ref{BesselEX3}).  Thus,
\[
u'' + x^{-1} u' + \left( k^2 x^2 - \frac{1}{4} x^{-2}\right) u = 0 \ .
\]
Since
\begin{align*}
\dydx{u}{x} &= \dydx{u}{z}\dydx{z}{x} = k x \dydx{u}{z}
= \sqrt{2kz}\, \dydx{u}{z}
\intertext{and}
\dydxn{u}{x}{2} &= \dfdx{\left(\dydx{u}{x}\right)}{x} 
= \dfdx{\left(\sqrt{2kz}\, \dydx{u}{z}\right)}{z}\dydx{z}{x}
= \sqrt{2k} \left(\frac{1}{2}z^{-1/2} \dydx{u}{z} + z^{1/2}
  \dydxn{u}{z}{2} \right) \sqrt{2kz} \\
&= 2k \left( \frac{1}{2} \dydx{u}{z} + z \dydxn{u}{z}{2}\right) \ ,
\end{align*}
where again we have used $y$ to denote $\tilde{y}$ defined by
$\tilde{y}(z) = y(x) = y(\sqrt{2z/k})$, we get
\begin{align*}
0 &= 2k \left( \frac{1}{2} \dydx{u}{z} + z \dydxn{u}{z}{2}\right) +
\sqrt{\frac{k}{2z}} \left( \sqrt{2kz} \, \dydx{u}{z}\right) +
\left( k^2 \left(\frac{2z}{k}\right)
- \frac{1}{4} \left(\frac{k}{2z}\right)\right) u \\
&= 2k z \dydxn{u}{z}{2} + 2 k \dydx{u}{z}
+ \left(2kz - \frac{k}{8z}\right)u \ .
\end{align*}
Thus,
\[
z^2 \dydxn{u}{z}{2} + z \dydx{u}{z}
+ \left(z^2 - \frac{1}{16}\right)u = 0 \ .
\]
This is a Bessel equation with $\nu = 1/4$.  Its general solution is
$\displaystyle u = A J_{1/4}(z) + B J_{-1/4}(z)$.  
Hence, the general solution of (\ref{BesselEX3}) is
$\displaystyle y(x) = \sqrt{x}\, u(x)
= A \sqrt{x} J_{1/4}\left(kx^2/2\right)
+ B \sqrt{x}\, J_{-1/4}\left(kx^2/2\right)$
for arbitrary constants $A$ and $B$.
\end{egg}

We have the following derivative rules for the Bessel functions of the
first kind. 

\begin{prop}
\begin{equation} \label{brr1}
(x^n J_{n}(x))' = x^n J_{n-1}(x)
\end{equation}
and
\begin{equation} \label{brr4}
(x^{-n} J_n(x))' = -x^{-n} J_{n+1}(x)
\end{equation}
for $n \ge 0$.
\end{prop}

\begin{proof}
It is easy to prove (\ref{brr1}) and (\ref{brr4}) using the power
series expansion of the Bessel function of the first kind.  From
\[
J_n(x) = \sum_{k=0}^{\infty} \, \frac{(-1)^k}{k! \Gamma(n+k+1)}
\left(\frac{x}{2}\right)^{n+2k} \ ,
\]
we get
\begin{align*}
\dfdx{(x^n J_n(x))}{x} &=
\dfdx{ \left(\sum_{k=0}^{\infty} \frac{(-1)^k x^{2n+2k}}
{k! \Gamma(n+k+1) 2^{n+2k}}\right) }{x}
= \sum_{k=0}^{\infty} \frac{(-1)^k (2n+2k) x^{2n+2k-1}}
{k! \Gamma(n+k+1) 2^{n+2k}} \\
&= \sum_{k=0}^{\infty} \frac{(-1)^k x^{2n+2k-1}}
{k! \Gamma(n+k) 2^{n+2k-1}}
= x^n \sum_{k=0}^{\infty} \frac{(-1)^k x^{n+2k-1}}
{k! \Gamma(n+k) 2^{n+2k-1}} = x^n J_{n-1}(x)
\end{align*}
and
\[
\dfdx{(x^{-n} J_n(x))}{x} =
\dfdx{ \left(\sum_{k=0}^{\infty} \frac{(-1)^k x^{2k}}
{k! \Gamma(n+k+1) 2^{n+2k}}\right) }{x}
= \sum_{k=1}^{\infty} \frac{(-1)^k (2k) x^{2k-1}}
{k! \Gamma(n+k+1) 2^{n+2k}} .
\]
If we substitute $k=s+1$ in the last equation, we get
\begin{align*}
\dfdx{(x^{-n} J_n(x))}{x} 
&= \sum_{s=0}^{\infty} \frac{(-1)^{s+1} x^{2s+1}(2s+2)}
{(s+1)! \Gamma(n+s+2) 2^{n+2s+2}}
= -x^{-n} \sum_{s=0}^{\infty} \frac{(-1)^s x^{2s+n+1}}
{s! \Gamma(n+s+2) 2^{n+2s+1}}\\
& = -x^{-n} J_{n+1}(x) \ . \qedhere
\end{align*}
\end{proof}

\begin{egg}
Compute the following integrals.               \label{ExBesselInt}
\[
\begin{array}{*{3}{l@{\hspace{0.5em}}l@{\hspace*{4em}}}}
\text{\bfseries a}) & \displaystyle \int x^4 J_1(x) \dx{x} 
& \text{\bfseries b}) & \displaystyle \int x^3 J_0(x) \dx{x}
& \text{\bfseries c}) & \displaystyle \int x J_2(x) \dx{x}
\end{array}
\]

\subQ{a} To compute this integral, we use integration by parts and
(\ref{brr1}).  Hence,
\begin{align*}
\int x^4 J_1(x) \dx{x}
&= \int x^2 \left( x^2 J_1(x) \right) \dx{x} \\
&= x^4 J_2(x) - \int 2 x^3 J_2(x) \dx{x}
\quad \left\{ 
\begin{array}{l}
\text{If we use integration by parts with} \\
f(x)=x^2 \text{ and } g'(x) = x^2 J_1(x), \text{ so} \\
f'(x) = 2x \text{ and } g(x) = x^2 J_2(x) \text{ from} \\
\text{(\ref{brr1}) with } n=2 .
\end{array} \right. \\
&= x^4 J_2(x) - 2 \int x^3 J_2(x) \dx{x} \\
&= x^4 J_2(x) - 2 x^3 J_3(x) + C \quad \Big\{
\text{From (\ref{brr1}) with } n = 3 . 
\end{align*}

\subQ{b} To compute this integral, we again use integration by parts and
(\ref{brr1}).  Hence,
\begin{align*}
\int x^3 J_0(x) \dx{x}
&= \int x^2  \left( x J_0(x)\right) \dx{x} \\
&= -x J_1(x) -  \int 2 x^2 J_1(x) \dx{x} \quad
\left\{
\begin{array}{l}
\text{If we use integration by parts with} \\
f(x)=x^2 \text{ and } g'(x) = x J_0(x), \text{ so} \\
f'(x) = 2x \text{ and } g(x) = x J_1(x) \text{ from} \\
\text{(\ref{brr1}) with } n=1 .
\end{array} \right. \\
&= x^3 J_1(x) - 2 \int x^2 J_1(x) \dx{x} \\
&= x^3 J_1(x) - 2 x^2 J_2(x) + C \quad
\Big\{ \text{From (\ref{brr1}) with } n = 2 .
\end{align*}

\subQ{c} To compute this integral, we use integration by parts and
(\ref{brr4}).  Hence,
\begin{align*}
\int x J_2(x) \dx{x}
&= \int x^2 \left( x^{-1} J_2(x) \right) \dx{x} \\
&= -x J_1(x) + \int 2 J_1(x) \dx{x}
\quad \left\{
\begin{array}{l}
\text{If we use integration by parts with} \\
f(x)=x^2 \text{ and } g'(x) = x^{-1} J_2(x), \text{ so} \\
f'(x) = 2x \text{ and } g(x) = - x^{-1} J_1(x) \text{ from} \\
\text{(\ref{brr4}) with } n=1 .
\end{array} \right. \\  
&= -x J_1(x) + 2 \int J_1(x) \dx{x} \\
&= -x J_1(x) - 2 J_0(x) + C \quad \Big\{
\text{From (\ref{brr4}) with } n = 0 .
\end{align*}
\end{egg}

The Bessel functions of the first kind satisfy the following
recurrence relations.
\begin{align}
n J_n(x) + x J_n'(x) &= xJ_{n-1}(x) \ , \label{brr2}\\
n J_n(x) - x J_n'(x) &= xJ_{n+1}(x) \ , \label{brr5}\\
J_{n-1}(x) + J_{n+1}(x) &= \frac{2n}{x} J_n(x) \label{brr3}
\intertext{and}
J_{n-1}(x) - J_{n+1}(x) &= 2 J_n'(x) \label{brr6} \ .
\end{align}
(\ref{brr2}) and (\ref{brr5}) comes from expanding (\ref{brr1}) and
(\ref{brr4}) respectively.  The sum of (\ref{brr2}) and
(\ref{brr5}) yields (\ref{brr3}), and the difference of (\ref{brr2})
and (\ref{brr5}) yields (\ref{brr6}).

Two special Bessel functions of the first kind are
$\displaystyle J_{1/2}(x) = \sqrt{2/(\pi x)}\, \sin(x)$ and
$\displaystyle J_{-1/2}(x) = \sqrt{2/(\pi x)}\, \cos(x)$.

Some of the zeros $\alpha_{n,0}$ of $J_0$ with the value of
$J_1$ at these points, and some of the zeros $\alpha_{n,1}$ of $J_1$
with the value of $J_0$ at these points, are listed in the following
table.  The values are rounded to four decimals.
\[
\begin{array}{|c|c|c|c|c|}
\hline
n & \alpha_{n,0} & J_1(\alpha_{n,0}) & \alpha_{n,1} & J_0(\alpha_{n,1}) \\
\hline
0 &  &  &  0.0000 & 1.0000 \\
1 & 2.4048 & 0.5191 & 3.8317 & -0.4028 \\
2 & 5.5201 & -0.3403 & 7.0156 & 0.3001 \\
3 & 8.6537 & 0.2715 & 10.1735 & -0.2497 \\
4 & 11.7915 & -0.2325 & 13.3237 & 0.2184 \\
5 & 14.9309 & 0.2065 & 16.4706 & -0.1965 \\
6 & 18.0711 & -0.1877 & 19.6159 & 0.1801 \\
7 & 21.2116 & 0.1733 &  &  \\
\hline
\end{array}
\]

The Bessel equation (\ref{spv_bess_equ}) can be associated to a
regular singular Sturm-Liouville problem.  More precisely, the
substitution $x= \sqrt{\lambda}\, t$ and $\nu = k \in \NN$, transforms
(\ref{spv_bess_equ}) into
\begin{equation}\label{spv_BesselSLF}
\dfdx{ \left( t \dydx{y}{t}\right)}{t} + \left( - \frac{k^2}{t} +
\lambda\, t\right)  y = 0
\end{equation}
for $0 < t < R$, where $R$ is determined by the problem under
consideration.  This is (\ref{spv_sturm_liou_equ}) with $r(t) = t$,
$\displaystyle q(t)= -k^2/t$,
$p(t)=t$. $a=0$ and $b=R$.  If we require that $y$ be bounded near
$0$ and $y(R)=0$, we get a regular singular Sturm-Liouville problem
whose eigenvalues are
$\displaystyle \lambda = \lambda_{n,k} = \left(\alpha_{n,k}/R\right)^2$,
where $n > 0$ and $\alpha_{n,k}$ is the $n^{th}$ zero of $J_k(x)$.
The eigenfunctions associated to $\lambda_{n,k}$ are multiple of
$\displaystyle J_k\left(\alpha_{n,k} t/R\right)$.
This comes from the fact that the bounded solution of
(\ref{spv_BesselSLF}) on $[0,R]$ are scalar multiple of
$\displaystyle y(t) = J_k\big(\sqrt{\lambda} t\big)$ and
$y(R) = 0$ implies that $\sqrt{\lambda}\, R = \alpha_{n,k}$.
We ignore $n=0$ for $k>0$ because
$\displaystyle J_k\left(\alpha_{0,k} t/R\right) = J_k\left(0\right) = 0$
for all $t$ cannot be an eigenfunction.  The set
$\displaystyle \left\{ J_k\left(\alpha_{n,k} t/R\right) : n \geq 1 \right\}$
is complete and orthogonal in $L^2[0,R]$, where the
scalar product is defined by
\[
\ps{f}{g} = \int_0^R f(t)g(t) \, t \dx{t} \ .
\]
The orthogonality is a consequence of the theory for the
Sturm-Liouville problems.  More precisely, the Bessel functions of the
first kind satisfy the following orthogonality relations.

\begin{prop}
\[
\int_0^R J_k\left(\frac{\alpha_{i,k} t}{R}\right)
J_k\left(\frac{\alpha_{j,k} t}{R}\right)\, t \dx{t} =
\begin{cases}
0 & \mbox{ if } i \neq j \\
\displaystyle \frac{R^2}{2} J_{k+1}^2(\alpha_{i,k}) & \mbox{ if } i = j
\end{cases}
\]
\end{prop}

\begin{proof}
The orthogonality comes from the theory of the Sturm-Liouville problems.
We only need to prove that
\begin{equation}\label{spv_nebi}
\int_0^R J_k^2\left(\frac{\alpha_{n,k} t}{R}\right)\, t \dx{t} =
\frac{R^2}{2} J_{k+1}^2(\alpha_{n,k}) \ .
\end{equation}

We use the fact that $\displaystyle y(t) = J_k(\gamma_{n,k}t)$ with
$\gamma_{n,k} = \alpha_{n,k}/R$ is a solution of
(\ref{spv_BesselSLF}), the Bessel equation in the Sturm-Liouville
form with the conditions $y$ bounded at the origin and $y(R)=0$.
More specifically, we have
\[
\dfdx{\left( t \dfdx{\left(J_k(\gamma_{n,k}t)\right)}{t} \right)}{t}
+ \left( -\frac{k^2}{t} + \gamma_{n,k}^2 t\right) J_k(\gamma_{n,k}t) = 0 \ .
\]
Multiplying both sides by
$\displaystyle 2 t \dfdx{\left( J_k(\gamma_{n,k}t)\right)}{t}$, we get
\[
2 t \dfdx{\left( J_k(\gamma_{n,k}t) \right)}{t}
\dfdx{\left( t \dfdx{\left(J_k(\gamma_{n,k}t)\right)}{t}\right)}{t}
+ 2\left( -k^2 + \gamma_{n,k}^2 t^2\right)
J_k(\gamma_{n,k}t)\, \dfdx{\left( J_k(\gamma_{n,k}t)\right)}{t} = 0 \ .
\]
Namely,
\[
\dfdx{\left( t \dfdx{\left(J_k(\gamma_{n,k}t)\right)}{t}\right)^2}{t} 
+ \left(- k^2 + \gamma_{n,k}^2 t^2\right)
\dfdx{ J_k^2(\gamma_{n,k}t)}{t} = 0 \ .
\]
Hence,
\begin{equation}\label{spv_neb}
\begin{split}
\left( t \dfdx{\left(J_k(\gamma_{n,k}t)\right)}{t}\right)^2\bigg|_0^R
& = \int_0^R
\dfdx{\left( t \dfdx{\left(J_k(\gamma_{n,k}t)\right)}{t}\right)^2}{t} \dx{t}\\
&= \int_0^R \left(k^2 - \gamma_{n,k}^2 t^2\right)
\dfdx{ J_k^2(\gamma_{n,k}t)}{t} \dx{t} \ .
\end{split}
\end{equation}

We first compute the left hand side of (\ref{spv_neb}).  From
(\ref{brr5}) with $x=\gamma_{n,k}t$, we get
\[
t \dfdx{\left( J_k(\gamma_{n,k}t) \right)}{t} 
= \gamma_{n,k}t J_k'(\gamma_{n,k}t)
= k J_k(\gamma_{n,k}t) - \gamma_{n,k}t J_{k+1}(\gamma_{n,k}t) \ .
\]
Therefore,
\[
\left( t \dfdx{\left(J_k(\gamma_{n,k}t)\right)}{t}\right)^2\bigg|_0^R
= \left( k J_k(\gamma_{n,k}t)
- \gamma_{n,k}t J_{k+1}(\gamma_{n,k}t) \right)^2\bigg|_0^R \\
= \alpha_{n,k}^2 J_{k+1}^2(\alpha_{n,k})
\]
because $\gamma_{n,k}R = \alpha_{n,k}$, $J_k(\alpha_{n,k})=0$ for
all $n$, and $J_k(0) = 0$ for $k>0$.

We now turn our attention on the right hand side of (\ref{spv_neb}).
Using integration by parts with
$\displaystyle f(t) = \left(k^2 - \gamma_{n,k}^2 t^2\right)$
and $\displaystyle g'(t) = \dfdx{\left( J_k^2(\gamma_{n,k}t)\right)}{t}$,
so $f'(t) = - 2\gamma_{n,k}^2 t$ and
$\displaystyle g(t) = J_k^2(\gamma_{n,k}t)$, we get
\begin{align*}
\int_0^R \left(k^2 - \gamma_{n,k}^2 t^2\right)
\dfdx{\left( J_k^2(\gamma_{n,k}t)\right)}{t} \dx{t} 
&= \left(k^2 - \gamma_{n,k}^2 t^2\right) J_k^2(\gamma_{n,k}t)\bigg|_0^R
+ \int_0^R 2\gamma_{n,k}^2 t J_k^2(\gamma_{n,k}t) \dx{t} \\
&= 2\gamma_{n,k}^2 \int_0^R t J_k^2(\gamma_{n,k}t) \dx{t}
= 2\frac{\alpha_{n,k}^2}{R^2 } \int_0^R J_k^2(\gamma_{n,k}t)\, t \dx{t} \ ,
\end{align*}
where again we have used $J_k(\alpha_{n,k})=0$ for all $n$ and
$J_k(0) = 0$ for $k>0$.

It follows form (\ref{spv_neb}) that
\[
\alpha_{n,k}^2 J_{k+1}^2(\alpha_{n,k}) = 
2\frac{\alpha_{n,k}^2}{R^2 } \int_0^R J_k^2(\gamma_{n,k}t)\, t \dx{t} \ ,
\]
and this gives (\ref{spv_nebi}) after simplification.
\end{proof}

Hence, the norm of
$\displaystyle J_k\left(\alpha_{n,k} t / R\right)$ on $[0,R]$ is
given by
\[
\left\| J_k\left(\frac{\alpha_{n,k} t}{R}\right) \right\|^2
= \int_0^R J_k^2\left(\frac{\alpha_{n,k} t}{R}\right) \, t \dx{t} =
\frac{R^2}{2} J_{k+1}^2 (\alpha_{n,k}) \ .
\]
For $k$ fix, the Fourier-Bessel expansion of $f$ on $]0,R[$ with
respect to the functions
$\displaystyle J_k\left(\alpha_{n,k} t/R\right)$ for $n \geq 1$ is
\[
f = \sum_{n=1}^{\infty}\, a_{n}
J_k\left(\frac{\alpha_{n,k} t}{R}\right)
\]
for $0 < t < R$, where
\[
a_n = \frac{2}{R^2 J_{k+1}^2(\alpha_{n,k})} \int_0^R f(t)
J_k\left(\frac{\alpha_{n,k} t}{R}\right) \, t \dx{t}
\]
for $n \geq 1$.  The convergence of the series is in $\displaystyle L^2[0,R]$.

\begin{egg}
Express $\displaystyle f(x) = x^2$ for $0<x<R$ in terms of the Bessel functions
$\displaystyle \left\{ J_0(\gamma_{n,0}x)\right\}_{n=0}^\infty$, where
$\gamma_{n,0} = \alpha_{n,0}/R$.

The coefficient of the Fourier-Bessel series are
\begin{align*}
a_n &= \frac{2}{R^2 J_{1}^2(\alpha_{n,0})} \int_0^R
t^3J_0(\gamma_{n,0} t) \dx{t}
= \frac{2}{R^2 J_{1}^2(\alpha_{n,0})} \left( \frac{1}{\gamma_{n,0}^4}
\int_0^{\alpha_{n,0}} s^3 J_0(s) \dx{s} \right) \\
&= \frac{2}{R^2 J_{1}^2(\alpha_{n,0})\gamma_{n,0}^4}
\left( s^3 J_1(s) - 2\, s^2 J_2(s) \right)\bigg|_0^{\alpha_{n,0}} \\ 
&= \frac{2 R^2}{J_{1}^2(\alpha_{n,0})\alpha_{n,0}^4}
\left( \alpha_{n,0}^3 J_1(\alpha_{n,0})
- 2\, \alpha_{n,0}^2 J_2(\alpha_{n,0}) \right)
\end{align*}
The second equality comes form the substitution $s=\gamma_{n,0}t$ in
the integral.  The integral of $s^3J_0(s)$ computed in
Example~\ref{ExBesselInt} (b) gives the third equality.  The
expression for $a_n$ can be further simplified using (\ref{brr3}) with
$n=1$.  We get
\[
  J_2(\alpha_{n,0}) = J_0(\alpha_{n,0}) + J_2(\alpha_{n,0})
  = \frac{2}{\alpha_{n,0}} J_1(\alpha_{n,0}) \ .
\]
Thus
\[
  a_n = \frac{2 R^2}{J_{1}^2(\alpha_{n,0})\alpha_{n,0}^4}
\left( \alpha_{n,0}^3 \, J_1(\alpha_{n,0})
  - 4 \alpha_{n,0} J_1(\alpha_{n,0}) \right)
= \frac{2R^2}{\alpha_{n,0}^3 J_{1}(\alpha_{n,0})} \left(
\alpha_{n,0}^2 - 4\right) \ . 
\]
\end{egg}

\section{Exercises}

Suggested exercises:

\begin{itemize}
\item In \cite{PinRub}: numbers 6.1 to 6.9 in Section 6.7.
\item in \cite{Str}: numbers 6, 7 and 13 in Section 10.5.
\item In \cite{Simm}: numbers 1, 2 and 8 in Section 26; all the numbers in
Sections 27 to 29, and 32 to 35.
\end{itemize}

%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "notes"
%%% End: 
